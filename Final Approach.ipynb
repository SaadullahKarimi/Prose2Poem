{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Approach.ipynb","provenance":[{"file_id":"1bdklLEJmybuuxY07y1SZDeEv_SS2fIlC","timestamp":1621003043700},{"file_id":"1qZ3NGZIpucBHbIQjHZc5hhmlc8yFbxVT","timestamp":1620964676868}],"collapsed_sections":["M2IoKrqSZNSO"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zgd-9HBU5ShJ"},"source":["# Requirements"]},{"cell_type":"code","metadata":{"id":"kDa6ObYswzz_","collapsed":true},"source":["!pip install datasets transformers\n","!pip install git+https://github.com/LIAAD/yake\n","!pip install hazm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6UFkzLivw7Tt"},"source":["import collections\n","import helper\n","from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n","cc = SmoothingFunction()\n","\n","import tensorflow as tf\n","from transformers import AutoModelForMaskedLM, AutoTokenizer\n","\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","import torch\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import yake\n","from itertools import chain\n","import nltk\n","import pickle\n","from statistics import mean\n","import math\n","import pandas as pd\n","from itertools import product\n","import sys\n","import ast\n","import string\n","import unicodedata\n","import random\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pandas as pd\n","import hazm\n","import gc\n","from termcolor import colored\n","from itertools import chain\n","from tqdm import tqdm\n","from transformers import BertTokenizer, BertModel\n","pd.set_option('display.max_rows', 50)\n","pd.set_option('display.max_colwidth', None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iF9iMWURxA7P"},"source":["\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","all_data_address = '.../ProsPoemParallelDataset_augmented.pickle'\n","indexed_vocabs_address = '.../indexed_vocabs.pickle'\n","freqs_path = '.../freqs.pickle'\n","adjacency_graph_adress = '.../AdjacencyPickle.pickle'\n","translator_output_address = '.../MultiHeadAttention_Poetry_17_cleaned_data.csv'\n","poetry_beam_addreass = '.../PoetryBeamN.pickle'\n","output_address = '.../Results/final_output_BeitBert.csv'\n","G_ghafie_address = '.../RhymeGraph.pickle'\n","motaradef_address = '.../motaradef.pickle'\n","rhymes_address = '.../rhymes.pickle'\n","unw_tokens_address = '.../unw_wokens.pickle'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfYx4eq3xUvR"},"source":["\n","poetryBeam = pd.read_pickle(poetry_beam_addreass)\n","rhymes = pd.read_pickle(rhymes_address)\n","all_unw_tokens = pd.read_pickle(unw_tokens_address)\n","G_ghafie = pd.read_pickle(G_ghafie_address)\n","freqs = pd.read_pickle(freqs_path)\n","G = pd.read_pickle(adjacency_graph_adress)\n","indexed_vocabs = pd.read_pickle(indexed_vocabs_address)\n","motaradef = pd.read_csv(motaradef_address)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHYYw5lT3PGe"},"source":["motaradef.loc[:, 'synonym'] = motaradef.loc[:, 'synonym'].apply(lambda x: ast.literal_eval(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdTl7ZH5G6_p"},"source":["pd.set_option('display.max_rows', 100)\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eOCcASneHGcn"},"source":["# Phase || Huristic "]},{"cell_type":"code","metadata":{"id":"9-DX6OhSHJ0P"},"source":["# GUIDE\n","\n","# R (Robaei - Ghazal - Ghaside) - D (Dobeiti) - GH (Ghete) - M (Masnavi)\n","\n","# R: \n","# O-----------    O-----------    \n","# O-----------     -----------    \n","\n","# D: \n","# O-----------    O-----------    \n","# O-----------    O-----------    \n","\n","# GH: \n","# O-----------     -----------    \n","# O-----------     -----------    \n","\n","# M: \n","# O-----------    O-----------    \n","# X-----------    X-----------    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZujdnUZUHJx-"},"source":["all_fdist = dict()\n","for key in list(indexed_vocabs.keys()):\n","  fdist = nltk.FreqDist(indexed_vocabs[key])\n","  all_fdist[key] = fdist\n","print(len(all_fdist))\n","\n","most_common = []\n","\n","\n","\n","top_n = 5\n","for key in list(all_fdist.keys()):\n","    most_common += [each[0] for each in all_fdist[key].most_common()[:top_n]]\n","most_common = list(set(most_common))\n","print(most_common, len(most_common))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKE84oLhHJvd"},"source":["# Mitra has modified this section-------------------------------------\n","\n","language = \"fa\"\n","max_ngram_size = 1\n","deduplication_thresold = 0.99\n","deduplication_algo = 'seqm'\n","windowSize = 1\n","features = None\n","\n","def extract_keys(corpus, numOfKeywords):\n","    kw_extractor = yake.KeywordExtractor(lan=language, \n","                                         n=max_ngram_size,\n","                                         dedupLim=deduplication_thresold, \n","                                         dedupFunc=deduplication_algo, \n","                                         windowsSize=windowSize, \n","                                         top=numOfKeywords, \n","                                         features=features)\n","    \n","    keywords = kw_extractor.extract_keywords(corpus)\n","    # the lower the score the more relevant the key\n","    keys = [key[0] for key in sorted(keywords, key =  lambda x: x[1])]\n","    return keys\n","def extract_single_keys(mapped, numOfKeywords):\n","  '''\n","  input sample: \n","  mapped = [\n","          ['کار', 'عارف', 'راست', 'کو', 'نه', 'احول', 'است', '،', 'چشم', 'او', 'از', 'کشت\\u200cهای', 'اول', 'است', '.'],\n","          ['ای', 'بسا', 'کفار', 'را', 'سودای', 'دین', '،', 'بند', 'او', 'ناموس', 'و', 'کبر', 'و', 'این', 'کار', '.']\n","  ]\n","\n","  output sample: [['کار', 'چشم', 'عارف'], ['دین', 'بند', 'کار']]\n","  '''\n","  single_keys = []\n","  for each in mapped:\n","    keys = extract_keys(' '.join(each), numOfKeywords)\n","    single_keys.append(keys)\n","  return single_keys\n","\n","\n","def extract_joint_keys(mapped, numOfKeywords):\n","  '''\n","  input sample: \n","  mapped = [\n","          ['کار', 'عارف', 'راست', 'کو', 'نه', 'احول', 'است', '،', 'چشم', 'او', 'از', 'کشت\\u200cهای', 'اول', 'است', '.'],\n","          ['ای', 'بسا', 'کفار', 'را', 'سودای', 'دین', '،', 'بند', 'او', 'ناموس', 'و', 'کبر', 'و', 'این', 'کار', '.']\n","  ]\n","\n","  output sample: [['چشم', 'دین', 'بند']]\n","  '''\n","  joint_keys = []\n","  mapped_joined = ' '.join([' '.join(each) for each in mapped])\n","  keys = extract_keys(mapped_joined, numOfKeywords)\n","  joint_keys.append(keys)\n","\n","  return joint_keys\n","\n","\n","def extract_all_keys(single_keys, joint_keys, numOfKeywords):\n","  '''\n","  input sample: \n","  [['کار', 'چشم', 'عارف'], ['دین', 'بند', 'کار']]   [['چشم', 'دین', 'بند']]\n","\n","  output sample: {'بند', 'دین', 'کار', 'چشم', 'عارف'}\n","  '''\n","  final_keys = []\n","  for each1 in single_keys + joint_keys:\n","    for each2 in each1:\n","      final_keys.append(each2)\n","  final_keys = set(final_keys)\n","  return final_keys\n","\n","def calculate_fks(single_keys, joint_keys, final_keys):\n","  '''\n","  input sample: {'بند', 'دین', 'کار', 'چشم', 'عارف'}\n","\n","  output sample: [2, 2, 2, 2, 1]\n","  '''\n","  single_joint = []\n","  for each in single_keys + joint_keys:\n","    for each2 in each:\n","      single_joint.append(each2)\n","  fks_tmp = {}\n","  for each in single_joint:\n","    if each in fks_tmp:\n","      fks_tmp[each] = fks_tmp[each] + 1\n","    else:\n","      fks_tmp[each] = 1\n","  fks = [fks_tmp[each] for each in final_keys]\n","  return fks\n","# find_synonyms is used to extract synonyms for a list of keys.\n","# Our objective here is to extract n synonyms for each key (where n is equal to fks[key])\n","# provided that each key's length is less than 10, is only a single word, and has a frequency of bigger than 0.\n","def find_synonyms(keys, fks):\n","\n","    synonyms = {}\n","    for each in keys:\n","        synonyms[each] = []\n","    for i, each in enumerate(keys):\n","        if each in motaradef['word'].to_list():\n","\n","            tmp = motaradef[motaradef['word'] == each]['synonym'].tolist()[0]\n","\n","\n","            #tmp_mtr = [each2[1:-1].strip() for each2 in tmp[0].split(', ')]\n","            # tmp_mtz = [each[1:-1].strip() for each in tmp[1].split(', ')]   \n","            #print()\n","            #print(tmp)\n","\n","            for each2 in tmp:\n","                #print(each2)\n","                this_freq = sum([freqs[str(i + 1)][each2] for i in range(10)])\n","                if len(each2) < 10 and len(each2.split()) == 1 and this_freq > 0:\n","                    synonyms[each].append(each2)\n","                    if fks[i] == len(synonyms[each]):\n","                        break\n","    return synonyms\n","\n","\n","# Similar to the above function, but only a single synonym is returned.\n","def find_one_synonym(all_keys, key):\n","\n","    #print()\n","    #print('finding one syn ', )\n","    #print(motaradef[motaradef['word'] == key]['synonym'].tolist())\n","\n","    #try:\n","    if key in motaradef.word.tolist():\n","\n","        tmp = motaradef[motaradef['word'] == key]['synonym'].tolist()[0]\n","    \n","        for each2 in tmp:\n","            #print(each2)\n","            this_freq = sum([freqs[str(i + 1)][each2] for i in range(10)])\n","            if len(each2) < 10 and len(each2.split()) == 1 and each2 not in all_keys and this_freq > 0:\n","                return each2\n","    return key\n","\n","\n","# find_keys_synonyms combines all keywords and synonyms\n","def find_keys_synonyms(keys, synonyms):\n","    return keys + [each2 for each in list(synonyms.values()) for each2 in each]\n","\n","\n","# This function calculates a keyword/synonym 's position\n","# Try Except is used to handle exceptions in case a keyword/synonym is OOV\n","# find_key_poses returns both positions and the updated keys_synonyms list (which has no OOV word in it)\n","def find_key_poses(keys_synonyms):\n","  key_poses = {}\n","  for each in keys_synonyms:\n","    try:\n","      key_poses[each] = calculate_position(each, list(key_poses.values()))\n","    except:\n","      key_poses[each] = -1\n","  key_poses_main = {}\n","  for each in list(key_poses.keys()):\n","    if key_poses[each] != -1:\n","      key_poses_main[each] = key_poses[each]\n","  return key_poses_main, list(key_poses_main.keys())\n","\n","\n","# find what keywords have been suggested per index\n","def find_indices_suggestions(key_poses):\n","  indices_suggestions = {}\n","  for i in range(10):\n","    indices_suggestions[str(i + 1)] = [each for each in list(key_poses.keys()) if key_poses[each] == i + 1]\n","  return indices_suggestions\n","\n","# In order to normalize indices_suggestions \n","# we either remove or add a keyword\n","\n","\n","\n","# d is a parameter which measures the \n","# total number of differences between keywords' indices\n","# E.g. => input: HELLO NONE MITRA NONE HOW ARE NONE YOU -> \n","# HELLO - MITRA = 2\n","# MITRA - HOW = 2\n","# HOW - ARE = 1\n","# ARE - YOU = 2\n","# d = 7\n","def calculate_d(inp, mask_token):\n","  d = 0\n","  tmp_idx = None\n","  for i in range(len(inp)):\n","    if inp[i] != mask_token:\n","      if tmp_idx == None:\n","        tmp_idx = i\n","      else:\n","        d += i - tmp_idx\n","        tmp_idx = i\n","  return d\n","\n","\n","# g is a parameter which measures how likely are \n","# two words to be adjacent in a mesra\n","# E.g. => input: MITRA NONE HOW ARE NONE YOU -> \n","# MITRA - HOW = 0.65\n","# MITRA - ARE = 0.49\n","# MITRA - YOU = 0.76\n","# HOW - ARE = 0.4\n","# HOW - YOU = 0.9\n","# ARE - YOU = 0.3\n","# g = mean([0.65, 0.49, 0.76, 0.4, 0.9, 0.3])\n","def check_duplicate(track_list, idx1, idx2):\n","  if idx1 == idx2:\n","    return False\n","  if str(idx1) + '_' + str(idx2) in track_list or str(idx2) + '_' + str(idx1) in track_list:\n","    return False\n","  return True\n","\n","\n","# This function checks if any of the two input words is among the top_n percent of most frequent words\n","def check_if_most_common(inp1, inp2):\n","    if inp1 in most_common or inp2 in most_common:\n","        return True\n","    return False\n","\n","\n","def calculate_g(words, mask_token):\n","  weights = []\n","  words = [each for each in words if each != mask_token]\n","  track_list = []\n","  for i in range(len(words)):\n","    for j in range(len(words)):\n","      if check_duplicate(track_list, i, j):\n","        track_list.append(str(i) + '_' + str(j))\n","        tmp_i, tmp_j = i + 1, j + 1\n","        if G.has_edge(words[i], words[j]) and not check_if_most_common(words[i], words[j]):\n","          weights.append(G[words[i]][words[j]]['weight'])\n","  if len(weights) > 0:\n","    return mean(weights)\n","  else:\n","    return 0\n","\n","def sigmoid(x):\n","  return 1 / (1 + math.exp(-x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXVUtytUHJtb"},"source":["# calculate n_r to quantify how many keywords (in total) are before (right-side) a specific index\n","def calculate_n_r(idx, previous_positions): \n","    # idx (1 - 10)\n","    counter = 0\n","    for each in previous_positions:\n","        if each < idx:\n","            counter += 1\n","    return counter + 1\n","\n","\n","# calculate n_l to quantify how many keywords (in total) are after (left-side) a specific index\n","def calculate_n_l(idx, previous_positions): \n","    # idx (1 - 10)\n","    counter = 0\n","    for each in previous_positions:\n","        if each > idx:\n","            counter += 1\n","    return counter + 1\n","\n","\n","# frn (first right neighbour) - fln (first left neighbour)\n","# calculate frn_d to measure the distance between the index to its first right-side neighbour\n","def calculate_frn_d(idx, previous_positions): \n","    # idx (1 - 10)\n","    counter = 0\n","    tmp_idx = idx - 1\n","    while tmp_idx != 0:\n","        counter += 1\n","        if tmp_idx in previous_positions:\n","            break\n","        tmp_idx -= 1\n","    return counter + 1, tmp_idx\n","\n","\n","# calculate fln_d to measure the distance between the index to its first left-side neighbour\n","def calculate_fln_d(idx, previous_positions): \n","    # idx (1 - 10)\n","    counter = 0\n","    tmp_idx = idx + 1\n","    while tmp_idx != 11:\n","        counter += 1\n","        if tmp_idx in previous_positions:\n","            break\n","        tmp_idx += 1\n","    return counter + 1, tmp_idx\n","\n","\n","# calculate frn_agg to quantify the aggregation of frn\n","def calculate_frn_agg(frn_idx, previous_positions): \n","    return previous_positions.count(frn_idx) + 1\n","\n","\n","# calculate fln_agg to quantify the aggregation of fln\n","def calculate_fln_agg(fln_idx, previous_positions): \n","    return previous_positions.count(fln_idx) + 1\n","\n","\n","# calculate rns (right neighbour score)\n","def calculate_rns(n_r, frn_d, frn_agg):\n","    return (1 / n_r) * (frn_d / frn_agg)\n","\n","    \n","# calculate lns (left neighbour score)\n","def calculate_lns(n_l, fln_d, fln_agg):\n","    return (1 / n_l) * (fln_d / fln_agg)\n","\n","\n","# calculate the aggregation at the candidate index\n","def calculate_idx_agg(idx, previous_positions): \n","    return previous_positions.count(idx) + 1\n","\n","\n","# Given a word, we sort the indices among which the word has been seen the most.\n","def calculate_position(each_key, previous_positions):\n","    one_to_ten = []\n","    for i in range(len(freqs.keys())):\n","        this_freqs = freqs[str(i + 1)][each_key]\n","        one_to_ten.append(this_freqs)\n","    one_to_ten_scored = []\n","\n","    for i, idx_freq in enumerate(one_to_ten):\n","\n","        idx = i + 1\n","        n_r = calculate_n_r(idx, previous_positions)\n","        frn_d, frn_d_idx = calculate_frn_d(idx, previous_positions)\n","        frn_agg = calculate_frn_agg(frn_d_idx, previous_positions)\n","        n_l = calculate_n_l(idx, previous_positions)\n","        fln_d, fln_d_idx = calculate_fln_d(idx, previous_positions)\n","        fln_agg = calculate_fln_agg(fln_d_idx, previous_positions)\n","        rns = calculate_rns(n_r, frn_d, frn_agg)\n","        lns = calculate_lns(n_l, fln_d, fln_agg)\n","        idx_agg = calculate_idx_agg(idx, previous_positions)\n","        the_score = 0\n","        if idx_freq != 0:\n","            the_score = (abs(rns - lns) / idx_agg) * math.log(idx_freq, 10)\n","        one_to_ten_scored.append([idx, the_score])\n","    one_to_ten_scored_sorted = sorted(one_to_ten_scored, key =  lambda x: x[1], reverse=True)\n","    return one_to_ten_scored_sorted[0][0]\n","    # if len(one_to_ten) < 2:\n","    #     max_value = max(one_to_ten)\n","    #     max_index = one_to_ten.index(max_value) \n","    # else:\n","    #     max_value = one_to_ten_sorted[random.randint(0, 1)]\n","    #     max_index = one_to_ten.index(max_value) \n","    # return max_index + 1\n","\n","\n","def clean_poetry(inp):\n","  poetry = inp.split()[1:-1]\n","  poetry = poetry[:poetry.index('<sep>')] + ['،'] + poetry[poetry.index('<sep>') + 1:]\n","  return poetry\n","\n","\n","\n","# In order to normalize indices_suggestions \n","# we either remove or add a keyword\n","\n","# This def removes a keyword from\n","# keys_synonyms and updates key_poses and \n","# indices_suggestions\n","def remove_keyword(inp, keys_synonyms):\n","    \n","  keys_synonyms.remove(inp)\n","  key_poses, keys_synonyms = find_key_poses(keys_synonyms)\n","  indices_suggestions = find_indices_suggestions(key_poses)\n","  #print(inp, 'DELETED')\n","  return keys_synonyms, key_poses, indices_suggestions\n","\n","# This def adds a keyword to keys_synonyms \n","# and updates key_poses and indices_suggestions\n","def add_keyword(inp, keys_synonyms):\n","    \n","  the_synonym = find_one_synonym(keys_synonyms, inp)\n","  keys_synonyms.append(the_synonym)\n","  key_poses, keys_synonyms = find_key_poses(keys_synonyms)\n","  indices_suggestions = find_indices_suggestions(key_poses)\n","  #print(the_synonym, 'ADDED')\n","  return keys_synonyms, key_poses, indices_suggestions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIgkAp1qHJrp"},"source":["\n","def rnd_rhyme_scorer(each, inp_keys, mask_token):\n","    score = 0\n","    #print(inp_keys)\n","    for each_key in inp_keys:\n","        if each_key != mask_token:\n","            #print(each_key)\n","            if G.has_edge(each, each_key):\n","                score += G[each][each_key]['weight']\n","    return score / len(inp_keys)\n","\n","\n","\n","all_possible_rhymes = [each2 for each in rhymes for each2 in each]\n","\n","\n","def find_rhyme(inp, rnd=False, rhymes = rhymes, inp_keys=[]):\n","    \n","    if rnd == False:\n","        for r, each_row in enumerate(rhymes):\n","            for w, each_word in enumerate(each_row):\n","\n","                if each_word == inp:\n","\n","                    #print('inp ', inp)\n","                    each_row_tmp = each_row[:w] + each_row[w + 1:]\n","                    each_row_tmp = [[each, rhyme_scorer(inp, each)] for each in each_row_tmp]\n","                    each_row_tmp = sorted(each_row_tmp, key=lambda x:x[1], reverse=True)\n","                    if len(each_row_tmp) > 0:\n","                        #print(each_row_tmp)\n","                        return each_row_tmp[0][0]\n","                    else:\n","                        return inp\n","    else:\n","        if len(inp_keys)>0:\n","            \n","\n","            if len(inp_keys[0])>0:\n","                #print('hello')\n","                \n","                all_possible_rhymes_scored = [[each, rnd_rhyme_scorer(each, inp_keys[0], mask_token)] for each in all_possible_rhymes]\n","                all_possible_rhymes_scored = sorted(all_possible_rhymes_scored, key=lambda x:x[1], reverse=True)\n","                #print(all_possible_rhymes_scored)\n","\n","\n","                return all_possible_rhymes_scored[0][0]\n","    return inp\n","\n","def mutual_check(idx, final_set, comb4, comb3, mask_token):\n","    if idx == 1:\n","        return comb4[0]\n","    if idx % 2 == 1:\n","        # its odd (right-side)\n","        final_set_words_used = [each[0] for each in final_set]\n","        comb4_scored = []\n","        for candidate_list in comb4:\n","            candidate_list_cleaned = [each_word for each_word in candidate_list[0] if each_word != mask_token]\n","            new_score_tmp = candidate_list[-1]\n","            for each in final_set_words_used:\n","                each_cleaned = [each_word for each_word in each if each_word != mask_token]\n","                mutual_num = len([w for w in candidate_list_cleaned if w in each_cleaned])\n","                new_score = candidate_list[-1] * (1 - (mutual_num / 4))\n","                if new_score < new_score_tmp:\n","                    new_score_tmp = new_score\n","            comb4_scored.append(candidate_list + [new_score_tmp])\n","        comb4_scored_sorted = sorted(comb4_scored, key=lambda x:x[-1], reverse=True)\n","        return comb4_scored_sorted[0][:-1]\n","    else:\n","        # its even (left-side)\n","        final_set_words_used = [each[0] for each in final_set]\n","        comb3_scored = []\n","        for candidate_list in comb3:\n","            candidate_list_cleaned = [each_word for each_word in candidate_list[0] if each_word != mask_token]\n","            new_score_tmp = candidate_list[-1]\n","            for each in final_set_words_used:\n","                each_cleaned = [each_word for each_word in each if each_word != mask_token]\n","                mutual_num = len([w for w in candidate_list_cleaned if w in each_cleaned])\n","                new_score = candidate_list[-1] * (1 - (mutual_num / 3))\n","                if new_score < new_score_tmp:\n","                    new_score_tmp = new_score\n","            comb3_scored.append(candidate_list + [new_score_tmp])\n","        comb3_scored_sorted = sorted(comb3_scored, key=lambda x:x[-1], reverse=True)\n","        return comb3_scored_sorted[0][:-1]\n","\n","\n","\n","def final_sequence(all_preds, candidate_seq, counter, format, mask_token, random = True):\n","    #print(candidate_seq)\n","    if format == 'M':\n","        # FEDIT\n","        last_non_mask_word_idx = max([index for index, element in enumerate(candidate_seq) if element != mask_token])\n","        if (counter + 1) % 2 == 1:\n","            if last_non_mask_word_idx >= 8:\n","                sequence = ' '.join(candidate_seq[0][:last_non_mask_word_idx + 1])\n","            else:\n","                sequence = ' '.join(candidate_seq[0][:last_non_mask_word_idx + 1]+ [mask_token, find_rhyme('', rnd=random, inp_keys=candidate_seq)])\n","        else:\n","            if candidate_seq[0][7] == mask_token:\n","                last_ghafie = all_preds[-1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                sequence = ' '.join(candidate_seq[0][:8] + [find_rhyme(last_ghafie)])\n","            else:\n","                last_ghafie = all_preds[-1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                sequence = ' '.join(candidate_seq[0][:8] + [mask_token, find_rhyme(last_ghafie)])\n","\n","    elif format == 'GH':\n","        last_non_mask_word_idx = max([index for index, element in enumerate(candidate_seq) if element != mask_token])\n","        if (counter + 1) % 2 == 1:\n","            if last_non_mask_word_idx >= 8:\n","                sequence = ' '.join(candidate_seq[0][:last_non_mask_word_idx + 1])\n","            else:\n","                sequence = ' '.join(candidate_seq[0][:7]+ [mask_token, find_rhyme('', rnd=random, inp_keys=candidate_seq)])\n","        else:\n","            if counter == 1:\n","                if candidate_seq[0][7] == mask_token:\n","                    last_ghafie = all_preds[-1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                    sequence = ' '.join(candidate_seq[0][:8] + [find_rhyme('', rnd=random, inp_keys=candidate_seq)])\n","                else:\n","                    last_ghafie = all_preds[-1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                    sequence = ' '.join(candidate_seq[0][:8] + [mask_token, find_rhyme('', rnd=random, inp_keys=candidate_seq)])\n","            else:\n","                if candidate_seq[0][7] == mask_token:\n","                    last_ghafie = all_preds[1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                    sequence = ' '.join(candidate_seq[0][:8] + [find_rhyme(last_ghafie)])\n","                else:\n","                    last_ghafie = all_preds[1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                    sequence = ' '.join(candidate_seq[0][:8] + [mask_token, find_rhyme(last_ghafie)])\n","    elif format == 'D':\n","        last_non_mask_word_idx = max([index for index, element in enumerate(candidate_seq) if element != mask_token])\n","        if counter == 0:\n","            if last_non_mask_word_idx >= 8:\n","                sequence = ' '.join(candidate_seq[0][:last_non_mask_word_idx + 1])\n","            else:\n","                sequence = ' '.join(candidate_seq[0][:last_non_mask_word_idx + 1]+ [mask_token, find_rhyme('', rnd=random, inp_keys=candidate_seq)])\n","        else:\n","            if candidate_seq[0][7] == mask_token:\n","                last_ghafie = all_preds[-1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                sequence = ' '.join(candidate_seq[0][:8] + [find_rhyme(last_ghafie)])\n","            else:\n","                last_ghafie = all_preds[-1][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                sequence = ' '.join(candidate_seq[0][:8] + [mask_token, find_rhyme(last_ghafie)])\n","    elif format == 'R':\n","        last_non_mask_word_idx = max([index for index, element in enumerate(candidate_seq) if element != mask_token])\n","        if (counter + 1) % 2 == 1:\n","            if last_non_mask_word_idx >= 8:\n","                sequence = ' '.join(candidate_seq[0][:last_non_mask_word_idx + 1])\n","            else:\n","                sequence = ' '.join(candidate_seq[0][:last_non_mask_word_idx + 1]+ [mask_token, find_rhyme('', rnd=random, inp_keys=candidate_seq)])\n","        else:\n","            if candidate_seq[0][7] == mask_token:\n","                last_ghafie = all_preds[0][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                sequence = ' '.join(candidate_seq[0][:8] + [find_rhyme(last_ghafie)])\n","            else:\n","                last_ghafie = all_preds[0][list(all_preds[-1].keys())[-1]][0].split()[-1]\n","                sequence = ' '.join(candidate_seq[0][:8] + [mask_token, find_rhyme(last_ghafie)])        \n","    return sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNQYQ72eAnVL"},"source":["# find_rhyme(inp = '',\n"," #rnd=True, \n","# inp_keys=[['وین', mask_token, mask_token, 'تعجب', 'حیرت', 'انبیا', mask_token, mask_token, mask_token, mask_token], 5, 0.675, 0.827398570285026])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0GzK_wX4HJpW"},"source":["# Final heuriscs function ------------------------------------------\n","\n","def heuristic(poetry, mask_token, numOfKeywords, mesra_first_elements_num, \n","              mesra_second_elements_num):\n","\n","    # first get the output of translator and clean it----------------\n","    mapped = [clean_poetry(poetry)]\n","    \n","    # keywords---------------------------------------------------------\n","    single_keys = extract_single_keys(mapped, numOfKeywords)\n","    joint_keys = extract_joint_keys(mapped, numOfKeywords)\n","    final_keys = extract_all_keys(single_keys, joint_keys, numOfKeywords)\n","    fks = calculate_fks(single_keys, joint_keys, final_keys)\n","    \n","\n","    # finding syns and their positions-----------------------------\n","    keys = list(final_keys)\n","    synonyms = find_synonyms(keys, fks)\n","    keys_synonyms = find_keys_synonyms(keys, synonyms)\n","    key_poses, keys_synonyms = find_key_poses(keys_synonyms)\n","    print(colored('\\nkey: ', 'red'), keys)\n","    print(colored('keys_synonyms: ', 'red'), keys_synonyms)\n","    \n","\n","    #print('key_poses:', key_poses)\n","    indices_suggestions = find_indices_suggestions(key_poses)\n","\n","    # number of beits-----------------------------------------\n","    beit_num = mean([len(each) for each in list(indices_suggestions.values())])\n","\n","    beit_num = math.ceil(beit_num)\n","    if beit_num>4:\n","        beit_num = 4\n","    mesra_num = beit_num * 2\n","\n","    # num of keys required------------------------------------\n","    fixed_num_keys_required = int(((mesra_num / 2) * mesra_first_elements_num) + ((mesra_num / 2) * mesra_second_elements_num))\n","\n","    # If the number of keywords that we \n","    # already have is more than the number of keywords\n","    #  we should have: \n","    # we remove the least frequent ones\n","    # from the most keyword-aggregated indices \n","\n","    if fixed_num_keys_required < len(keys_synonyms):\n","        difference = len(keys_synonyms) - fixed_num_keys_required\n","        # print(difference, 'more than needed.')\n","        deletion_noms = sorted([[key, len(value)] for key, value in list(indices_suggestions.items())], key = lambda x: x[1], reverse=True)\n","        deletion_noms = deletion_noms[:difference]\n","\n","        for ii in range(len(deletion_noms)):\n","            each = deletion_noms[0]\n","            idx = each[0]\n","            deletion_noms_per_idx = sorted([[key, all_fdist[idx][key]] for key in indices_suggestions[idx]], key = lambda x: x[1], reverse=False)\n","            #print()\n","            #print('deletion_noms', deletion_noms)\n","            #print('deletion_noms_per_idx', deletion_noms_per_idx)\n","            #print('indices_suggestions', indices_suggestions)\n","            #print('keys_synonyms', keys_synonyms)\n","            #print()\n","            keys_synonyms, key_poses, indices_suggestions = remove_keyword(deletion_noms_per_idx[0][0], keys_synonyms)\n","            deletion_noms = sorted([[key, len(value)] for key, value in list(indices_suggestions.items())], key = lambda x: x[1], reverse=True)\n","            deletion_noms = deletion_noms[:difference]\n","\n","\n","    # If the number of keywords that\n","    # we already have is less than the number of keywords \n","    # we should have: \n","    # we add synonyms of the most frequent keywords in \n","    # the least keyword-aggregated indices \n","\n","\n","    # Mitra has modified > to > ----------------------------------------------\n","    \n","    elif fixed_num_keys_required >= len(keys_synonyms):\n","        difference = fixed_num_keys_required - len(keys_synonyms)\n","        # print(difference, 'less than needed.')\n","        # print('indices_suggestions:', indices_suggestions)\n","        # addition_noms = sorted([[key, len(value)] for key, value in list(indices_suggestions.items()) if len(value) > 0], key = lambda x: x[1], reverse=False)\n","        # # print('addition_noms before truncating:', addition_noms)\n","        # addition_noms = addition_noms[:difference]\n","        # # print('after:', addition_noms)\n","        # # print('addition_noms:', addition_noms)\n","        # for each in addition_noms:\n","        #     idx = each[0]\n","        #     # print('idx:', idx)\n","            \n","        #     addition_noms_per_idx = sorted([[key, all_fdist[idx][key]] for key in indices_suggestions[idx]], key = lambda x: x[1], reverse=True)\n","        #     # print(colored('adding', 'blue'), addition_noms_per_idx)\n","        #     #for m in range(len()):\n","        #      #   keys_synonyms, key_poses, indices_suggestions = add_keyword(addition_noms_per_idx[0][0])\n","\n","        addition_noms = sorted([[key, len(value)] for key, value in list(indices_suggestions.items()) if len(value) > 0], key = lambda x: x[1], reverse=False)\n","        addition_noms = addition_noms[:difference]\n","\n","        for ii in range(len(addition_noms)):\n","            each = addition_noms[0]\n","            idx = each[0]\n","            addition_noms_per_idx = sorted([[key, all_fdist[idx][key]] for key in indices_suggestions[idx]], key = lambda x: x[1], reverse=True)\n","            #print()\n","            #print('addition_noms', addition_noms)\n","            #print('addition_noms_per_idx', addition_noms_per_idx)\n","            #print('indices_suggestions', indices_suggestions)\n","            #print('keys_synonyms', keys_synonyms)\n","            #print()\n","            keys_synonyms, key_poses, indices_suggestions = add_keyword(addition_noms_per_idx[0][0], keys_synonyms)\n","            addition_noms = sorted([[key, len(value)] for key, value in list(indices_suggestions.items()) if len(value) > 0], key = lambda x: x[1], reverse=False)\n","            addition_noms = addition_noms[:difference]\n","        \n","    # finding possible combinitions-----------------------\n","    # Let's find out all possible combinations of keywords among the 10 indices\n","    # In order not to necessarily choose a keyword from an index, we add 'NONE' to all indices\n","    indices_contents = []\n","    for each in indices_suggestions.items():\n","        indices_contents.append(each[1] + [mask_token])\n","    \n","    \n","    # Here we calculate all possible combinations\n","    possible_combinations = list(product(*indices_contents))\n","    print(colored('\\nlen(possible_combinations) = ', 'red'), len(possible_combinations))\n","\n","\n","    # Here we choose those combinations in which mesra_first_elements_num (4) \n","    # and mesra_second_elements_num (3) number of keywords are seen\n","    possible_combinations_4, possible_combinations_3 = [], []\n","    for each in possible_combinations:\n","        each = list(each)\n","        if each.count(mask_token) == (10 - mesra_first_elements_num):\n","            possible_combinations_4.append(each)\n","        if each.count(mask_token) == (10 - mesra_second_elements_num):   \n","            possible_combinations_3.append(each) \n","    \n","    \n","    # Let's measure d, g, and sig for each combination\n","    possible_combinations_4_rated = {}\n","    for i in range(len(possible_combinations_4)):\n","        # info_dict contains d (index = 0) and g (index = 1) and sig (index = 2)\n","        d = calculate_d(possible_combinations_4[i], mask_token)\n","        g = calculate_g(possible_combinations_4[i], mask_token)\n","        sig = sigmoid(math.log(d, 2) * g)\n","        info_dict = [d, g, sig]\n","        possible_combinations_4_rated[i] = info_dict\n","\n","    possible_combinations_3_rated = {}\n","    for i in range(len(possible_combinations_3)):\n","        # info_dict contains d (index = 0) and g (index = 1) and sig (index = 2)\n","        d = calculate_d(possible_combinations_3[i], mask_token)\n","        g = calculate_g(possible_combinations_3[i], mask_token)\n","        sig = sigmoid(math.log(d, 2) * g)\n","        info_dict = [d, g, sig]\n","        possible_combinations_3_rated[i] = info_dict\n","\n","    # Let's zip both the suggestions and their respective d, g, and sig.\n","    final_combinations_4 = []\n","    for i in range(len(possible_combinations_4)):\n","        d, g, sig = possible_combinations_4_rated[i]\n","        final_combinations_4.append([possible_combinations_4[i], d, g, sig])\n","\n","    final_combinations_3 = []\n","    for i in range(len(possible_combinations_3)):\n","        d, g, sig = possible_combinations_3_rated[i]\n","        final_combinations_3.append([possible_combinations_3[i], d, g, sig])\n","\n","    # Let's sort all combinations by their sigmoid value\n","    \n","    final_combinations_4_sorted =  sorted(final_combinations_4, key = lambda x: x[-1], reverse=True)\n","    final_combinations_3_sorted =  sorted(final_combinations_3, key = lambda x: x[-1], reverse=True)\n","\n","    # Let's make our the_final_set which is made up of top 4 and 3 combinations.\n","    the_final_set = []\n","    if len(final_combinations_4_sorted) + len(final_combinations_3_sorted) < mesra_num or len(final_combinations_4_sorted)  == 0 or len(final_combinations_3_sorted) == 0:\n","        \n","        the_final_set = []\n","        for i in range(len(possible_combinations)):\n","            d = calculate_d(possible_combinations[i], mask_token)\n","            g = calculate_g(possible_combinations[i], mask_token)\n","            try:\n","                sig = sigmoid(math.log(d, 2) * g)\n","            except:\n","                sig = 0\n","            the_final_set.append([list(possible_combinations[i]), d, g, sig])\n","            \n","        the_final_set =  sorted(the_final_set, key = lambda x: x[-1], reverse=True)\n","\n","    else:\n","        for i in range(mesra_num):\n","            the_final_set.append(mutual_check(i + 1, the_final_set, final_combinations_4_sorted, final_combinations_3_sorted, mask_token)) \n","\n","\n","\n","    # output masked poetry\n","    # poetries_with_msk = [' '.join(each[0]) for each in the_final_set]\n","    #poetries_with_msk = [each[0] for each in the_final_set]\n","\n","    return the_final_set\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ezQ9L8LeMWPs"},"source":["# FEDIT\n","def rhyme_scorer(inp, candidate):\n","    if G_ghafie.has_edge(inp, candidate):\n","        return G_ghafie[inp][candidate]['weight']\n","    else:\n","        return 0\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoN2rJA0iLoT"},"source":["type(list(poetryBeam['F'].values()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIn6v2tAhvQS"},"source":["for i in list(poetryBeam['B'].keys())[:20]:\n","    print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DTjdoqp2SNQj"},"source":["def underline_join(words):\n","    return '_'.join(words)\n","\n","def beam_scorer(lastWords, poetryBeam):\n","    score = 0\n","    if len(lastWords) == 4:\n","        sequence = underline_join(lastWords[:4])\n","        if sequence in poetryBeam['F']:\n","            score += 3 * poetryBeam['F'][sequence]\n","        sequence = underline_join(lastWords[:3])\n","        if sequence in poetryBeam['T']:\n","            score += 2 * poetryBeam['T'][sequence]\n","        sequence = underline_join(lastWords[:2])\n","        if sequence in poetryBeam['B']:\n","            score += 1 * poetryBeam['B'][sequence]\n","    elif len(lastWords) == 3:\n","        sequence = underline_join(lastWords[:3])\n","        if sequence in poetryBeam['T']:\n","            score += 4 * poetryBeam['T'][sequence]\n","        sequence = underline_join(lastWords[:2])\n","        if sequence in poetryBeam['B']:\n","            score += 2 * poetryBeam['B'][sequence]\n","    elif len(lastWords) == 2:    \n","        sequence = underline_join(lastWords[:2])\n","        if sequence in poetryBeam['B']:\n","            score += 6 * poetryBeam['B'][sequence]\n","    return score/6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_RcpVeYHJm2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PvQW0EVQeo1"},"source":["# Phase ||| MLM"]},{"cell_type":"code","metadata":{"id":"_-j4B_xmbTKe"},"source":["def import_model(model_name='BERT', on='beit', is_roberta = False):\n","\n","\n","    \"\"\"\n","    import the models based on their checkpoints and training parameters\n","    \"\"\"\n","    \n","\n","    if on == 'beit':\n","        if model_name=='BERT V2':\n","            model_folder_path = r'.../Pretrained Models/Pretrained on beit/BERT_V2_0.4_beit/'\n","        elif  model_name=='BERT V3':\n","            model_folder_path = r'.../Pretrained Models/Pretrained on beit/BERT_0.4_beit/'\n","        elif  model_name=='Roberta':\n","            is_roberta = True\n","            model_folder_path = '.../Pretrained Models/Pretrained on beit/Roberta_0.4_beit/'\n","        elif  model_name=='Distilbert':\n","            model_folder_path = '.../Pretrained Models/Pretrained on beit/DistilBERT_0.4_beit/'\n","        elif  model_name=='Albert':\n","            model_folder_path = '.../Pretrained Models/Pretrained on beit/Albert_0.4_beit'\n","        \n","        \n","        \n","\n","\n","    else:\n","\n","\n","        if model_name=='BERT V3':\n","\n","            model_folder_path = '.../Pretrained Models/Pretrained on verses/BERT_0.15_Verse/with 5 epochs/'\n","            \n","        elif  model_name=='Albert':\n","\n","            model_folder_path = '.../Pretrained Models/Pretrained on verses/Albert_0.15_Verse/with 5 epochs/'\n","            \n","        elif  model_name=='Roberta':\n","\n","            is_roberta = True\n","            model_folder_path = '.../Pretrained Models/Pretrained on verses/Roberta_0.15_Verse/with 5 epochs/'\n","            \n","        elif  model_name=='Distilbert':\n","\n","            model_folder_path = '.../Pretrained Models/Pretrained on verses/DistilBERT_0.15_Verse/with 5 epochs/'\n","            \n","\n","\n","    model = AutoModelForMaskedLM.from_pretrained(model_folder_path)\n","    tokenizer = AutoTokenizer.from_pretrained(model_folder_path, use_fast=True)\n","    mask_token = tokenizer.mask_token\n","\n","    return mask_token, model, tokenizer, is_roberta\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIfTOWu2gXXM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLxR-GvHetOP"},"source":["def get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens):\n","\n","\n","    \"\"\"\n","    disallowing the model to generate very short tokens and punctuations\n","    \"\"\"\n","    unw = []\n","\n","    for v in tokenizer.vocab.keys():\n","        if len(v)<2:\n","            #print(v)\n","            unw.append(v)\n","\n","            \n","    banned_tokens = ['«', ':', '،', '/', '*', ']', '[', '؟', '…', 'ی', tokenizer.unk_token] + unw\n","    # these two have meanings in farsi so we won't remove them\n","    # و = and \n","    # ز = from\n","    all_banned_tokens = list(set(banned_tokens).union(all_unw_tokens)-set(['ز', 'و']))\n","    all_banned_tokens = [i.strip() for i in all_banned_tokens] \n","\n","    if is_roberta == True :\n","        all_banned_tokens = all_banned_tokens + ['/ ', ' /']\n","                                \n","\n","\n","    banned_ids = []\n","\n","    for i in tokenizer.batch_encode_plus(all_banned_tokens, add_special_tokens=False).input_ids:\n","        if len(i)==1:\n","            banned_ids.append(i[0])\n","\n","        elif len(i)>1:\n","            banned_ids.append(i[1])\n","\n","    print('Total length of unwanted tokens: ', len(banned_ids))\n","\n","    return banned_ids\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDqzdGbjSYs-"},"source":["# Final Function\n"]},{"cell_type":"code","metadata":{"id":"r0F7IT1DUF0B"},"source":["# importing from the translator\n","\n","translator_output = pd.read_csv('.../Results/MultiHeadAttention_Poetry_17_withAugmented_P&T.csv') \n","\n","translator_output.dropna(inplace=True)\n","translator_output.reset_index(inplace=True, drop=True)\n","\n","poetry_mha = translator_output.loc[:, 'poetry_generated_MHA'].values.tolist()\n","poetry_gt = translator_output.loc[:, 'poetry_ground_truth'].values.tolist()\n","\n","for i in poetry_mha:\n","    if type(i) != str:\n","        print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-y3f4IQbWTE"},"source":["\n","def get_transformer_suggestions(sequence, model, tokenizer, banned_ids, top_k=5):\n","\n","    #sequence = mask_seq(sequence, model, tokenizer)\n","\n","    ids_main = tokenizer.encode(sequence,\n","                            return_tensors=\"pt\",\n","                            add_special_tokens=True)\n","    #print(ids_main)\n","    ids_ = ids_main.detach().clone()\n","    position = torch.where(ids_main == tokenizer.mask_token_id)\n","\n","    positions_list = position[1].numpy().tolist()\n","\n","\n","\n","    suggestion = []\n","    #print(positions_list)\n","    model_logits = model(ids_)['logits'][0][positions_list[0]]\n","\n","    # setting inf to the '/' token\n","    \n","    #print(model_logits)\n","    model_logits[banned_ids] = -math.inf\n","    \n","    \n","    top_k_tokens = torch.topk(model_logits, top_k, dim=0).indices.tolist()\n","    \n","    for j in range(len(top_k_tokens)):\n","\n","        suggestion.append(tokenizer.decode(top_k_tokens[j]))\n","\n","    \n","    return suggestion      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wScj2VlnGnnd"},"source":["\n","# BERT ADDED\n","\n","\n","def beam_search(raw_sentence , model, tokenizer, banned_ids, beam_depth=3, \n","                top_k=5, mask_token='[MASK]', end_token='<END>', start_token='<START>'):\n","\n","    raw_sentence_for_bert = raw_sentence.replace('<SEP>', '/')\n","\n","    leadings = [[[start_token], 0]]\n","    mask_counter = 0\n","\n","    #print(tokenizer_bert.tokenize(raw_sentence))\n","    for i, step in enumerate(raw_sentence.split()[1:]):\n","\n","\n","        #print('STEP', step)\n","        if step == mask_token:\n","            all_candidates = []\n","            for lead in leadings:\n","                lead_thread, lead_score = lead\n","\n","                lead_thread = [m.strip() for m in lead_thread]\n","\n","\n","                # BERT -----------------------------------------------------\n","\n","                # change this part when using distilbert\n","                # mask_seq\n","                #print(colored(i, 'green'))\n","                if i == 0: \n","                    transformer_inp = ' '.join(raw_sentence_for_bert.split()[i+1:-1])\n","                    #print('BERT INPUT', transformer_inp)\n","\n","                else :\n","                    #print('lead_thread', lead_thread)\n","                    transformer_inp = ' '.join(lead_thread[1:] + raw_sentence_for_bert.split()[i+1:-1])\n","                    #print('BERT INPUT', transformer_inp)\n","                #print(colored('\\nBERT INPUT: ', 'green'), transformer_inp)\n","                \n","                candidates = get_transformer_suggestions(transformer_inp,\n","                            model= model, \n","                            tokenizer=tokenizer, \n","                            top_k=5,\n","                            banned_ids=banned_ids)\n","                \n","                candidates = [ca.strip() for ca in candidates]\n","                #print('candidates', candidates)\n","\n","                # ------------------------------------------------------------\n","                for j, cand in enumerate(candidates):\n","\n","                    new_thread = lead_thread[-3:] + [cand]\n","                    #print(new_thread)\n","                    new_score = beam_scorer(new_thread, poetryBeam)\n","                    #print('new_score', new_score)\n","                    # new_score = j\n","\n","\n","                    #print('cand:{} score: {}'.format(cand, new_score))\n","                    all_candidates.append([lead_thread + [cand], new_score])\n","                    #print(candidates)\n","\n","                    all_candidates = sorted(all_candidates, key=lambda x:x[1], reverse=True)\n","                    #print('all_candidates sorted ', all_candidates)\n","                    #print(new_thread, new_score)\n","\n","            # print('old leadings', leadings)\n","            leadings = all_candidates[:beam_depth]\n","            # print('all_candidates', all_candidates)\n","            # print('new leadings', leadings)\n","\n","            mask_counter += 1\n","        elif step == end_token:\n","            new_score = 1\n","            # print('old leadings', leadings)\n","            for i in range(len(leadings)):\n","                # print(each_lead[0][-3:] + ['<END>'])\n","                new_score = beam_scorer(each_lead[0][-3:] + [end_token], poetryBeam)\n","                # new_score = 1\n","                leadings[i][1] += new_score\n","                leadings = sorted(leadings, key=lambda x:x[1], reverse=True)\n","            #print('new leadings', leadings)\n","        else:\n","            # append step to all leading threads\n","            for each_lead in leadings:\n","                each_lead[0].append(step)\n","\n","    # print('\\n', ' '.join(leadings[0][0]))\n","    return ' '.join(leadings[0][0][1:])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtOWUv5CbZk8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"upPbW36NfhJ9"},"source":["\n","def clean_mha(txt):\n","\n","    txt = re.sub('<sep>', ' / ', txt)\n","    txt = re.sub('(<start>)|(<end>)', '', txt)\n","\n","    return txt\n","\n","def clean_fpv(txt):\n","    remove_ = [' *\\[ZWNJ\\] *', '<START>']\n","    txt = re.sub(' ##', ' ', txt)\n","    txt = re.sub(' <SEP> ', ' / ', txt)\n","    for patt in remove_:\n","        txt = re.sub(patt, ' ', txt)\n","\n","\n","    final_txt = [txt.split()[0]]\n","\n","    for t in txt.split()[1:]:\n","        if t != final_txt[-1]:\n","            final_txt.append(t)\n","    \n","\n","    return ' '.join(final_txt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiG68ElLY7hM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5k1PETVOoXR"},"source":["\n","results_dict = {\n","    'poetry_format': [],\n","    'numOfKeywords': [],\n","    'mesra_first_elements_num': [],\n","    'mesra_second_elements_num' : [],\n","    'address': [],\n","    'beam_depth': []\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kC8KGE4bZh_"},"source":["def fill_masks_on_beit(model, tokenizer, translator_output,\n","               numOfKeywords, mesra_first_elements_num, \n","                mesra_second_elements_num, banned_ids,\n","               m = 10, poetry_format = 'M', beam_depth = 3, \n","               model_name ='Distilbert',\n","               output_folder = '.../Results/Experiment 2/DistilBERT/'\n","):\n","    \n","    \n","    \"\"\"\n","    create the output as a csv file with the\n","    ground truth\n","    poetry generated by the multiheadattention translator\n","    poetry generated after choosing keywords and filling the masks\n","    the output of heuristics\n","    \"\"\"\n","\n","\n","    # define the output format and address\n","\n","    address = output_folder +f'Final_{model_name}_{poetry_format}_poetry_format_{numOfKeywords}_numOfKeywords_{mesra_first_elements_num}_mesra_first_elements_num_{mesra_second_elements_num}_mesra_second_elements_num_{beam_depth}_beam_depth_{m}_first.csv'\n","\n","    results_dict['numOfKeywords'].append(numOfKeywords)\n","    results_dict['mesra_first_elements_num'].append(mesra_first_elements_num)\n","    results_dict['mesra_second_elements_num'].append(mesra_second_elements_num)\n","    results_dict['address'].append(address)\n","    results_dict['poetry_format'].append(poetry_format)\n","    results_dict['beam_depth'].append(beam_depth)\n","\n","    df_output_beit = {\n","        'poetry_ground_truth': poetry_gt[:m],\n","        'poetry_generated_MHA': poetry_mha[:m],\n","        'final_predicted_verses' : [],\n","        'heuristics': []\n","        }\n","\n","    # go through the dataset\n","\n","    mask_token = tokenizer.mask_token\n","\n","    for i in tqdm(range(0, len(poetry_mha[:m])), position=0, leave=True):\n","        \n","        # first get the heuristic masked seqs\n","        masked_sequences = heuristic(poetry_mha[i], mask_token, \n","                                     numOfKeywords, mesra_first_elements_num, \n","                                     mesra_second_elements_num)\n","        #print('after masked seq')\n","\n","\n","        # processing each verse and them giving the whole\n","        # beit to transformers and beamsearch\n","        all_preds, all_preds_tmp = [], []\n","\n","        for j in range(int(len(masked_sequences) / 2)):\n","        \n","            j *= 2\n","\n","            m1, m2 = masked_sequences[j], masked_sequences[j+1]\n","\n","\n","            try:\n","                # verse 1\n","                m1_final = final_sequence(all_preds_tmp, m1, j, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m1_final]})\n","\n","                # verse 2\n","                m2_final = final_sequence(all_preds_tmp, m2, j + 1, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m2_final]})\n","\n","\n","                \n","                all_preds.append(beam_search('<START> ' + m1_final + ' <SEP> ' + m2_final + ' <END>', \n","                                            beam_depth = beam_depth, model=model, tokenizer=tokenizer, top_k=5,\n","                                            banned_ids = banned_ids, mask_token=mask_token,\n","                                             end_token='<END>', start_token='<START>'))\n","\n","                \n","\n","            except: \n","\n","                m1_final = final_sequence(all_preds_tmp, m1, j, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m1_final]})\n","\n","                m2_final = final_sequence(all_preds_tmp, m2, j + 1, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m2_final]})\n","\n","                print(colored('\\nEXCEPTION!!!', 'red'))\n","\n","                print([ ' '.join(i[0]) for i in masked_sequences])\n","                \n","                all_preds.append('Not a sequence')\n","                df_output_beit['heuristics'].append(m1_final + ' / ' + m2_final)\n","            \n","            \n","\n","        df_output_beit['heuristics'].append([m[0].replace(mask_token, '_') for m in [list(n.values())[0] for n in all_preds_tmp]])\n","        \n","        df_output_beit['final_predicted_verses'].append(all_preds)\n","\n","        print('\\n')\n","        print(colored('heuristics: ', 'blue'), df_output_beit['heuristics'][i])\n","        print(colored('all_preds: ', 'green'), df_output_beit['final_predicted_verses'][i])\n","\n","\n","\n","    # cleaning the final file\n","\n","    df_output_beit = pd.DataFrame(df_output_beit)\n","    df_output_beit.loc[:, 'final_predicted_verses'] = df_output_beit.loc[:, 'final_predicted_verses'].apply(lambda x : [clean_fpv(i) for i in x])\n","\n","\n","    df_output_beit.loc[:, 'poetry_generated_MHA'] = df_output_beit.loc[:, 'poetry_generated_MHA'].apply(lambda x: clean_mha(x))\n","\n","    \n","    df_output_beit.to_csv(address, index=False)\n","\n","    return df_output_beit, address\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHq1P0AKJ9VP"},"source":["def fill_masks_on_verse(model, tokenizer, translator_output,\n","                        numOfKeywords, mesra_first_elements_num, \n","                        mesra_second_elements_num, banned_ids,\n","                        m = 10, poetry_format = 'M', beam_depth = 2, \n","                        model_name ='Distilbert', \n","                        output_folder = '.../Results/Experiment 2/DistilBERT/'):\n","    \n","    \n","    \"\"\"\n","    generate tokens for masks in each verse\n","    \"\"\"\n","\n","\n","    # define the output format and address\n","\n","    address = output_folder +f'Final_{model_name}_Verse_{poetry_format}_poetry_format_{numOfKeywords}_numOfKeywords_{mesra_first_elements_num}_mesra_first_elements_num_{mesra_second_elements_num}_mesra_second_elements_num_{beam_depth}_beam_depth_{m}_first.csv'\n","\n","    results_dict['numOfKeywords'].append(numOfKeywords)\n","    results_dict['mesra_first_elements_num'].append(mesra_first_elements_num)\n","    results_dict['mesra_second_elements_num'].append(mesra_second_elements_num)\n","    results_dict['address'].append(address)\n","    results_dict['poetry_format'].append(poetry_format)\n","    results_dict['beam_depth'].append(beam_depth)\n","\n","    df_output_beit = {\n","        'poetry_ground_truth': poetry_gt[:m],\n","        'poetry_generated_MHA': poetry_mha[:m],\n","        'final_predicted_verses' : [],\n","        'heuristics': []\n","        }\n","\n","    # go through the dataset\n","\n","    mask_token = tokenizer.mask_token\n","\n","    for i in tqdm(range(0, len(poetry_mha[:m])), position=0, leave=True):\n","        \n","        # first get the heuristic masked seqs\n","        masked_sequences = heuristic(poetry_mha[i], mask_token, \n","                                     numOfKeywords, mesra_first_elements_num, \n","                                     mesra_second_elements_num)\n","        #print('after masked seq')\n","\n","\n","        # processing each verse and them giving the whole\n","        # beit to transformers and beamsearch\n","        all_preds, all_preds_tmp = [], []\n","\n","        for j in range(int(len(masked_sequences) / 2)):\n","        \n","            j *= 2\n","\n","            m1, m2 = masked_sequences[j], masked_sequences[j+1]\n","\n","\n","            # verse 1\n","            m1_final = final_sequence(all_preds_tmp, m1, j, poetry_format, mask_token)\n","            all_preds_tmp.append({'0':[m1_final]})\n","\n","            # verse 2\n","            m2_final = final_sequence(all_preds_tmp, m2, j + 1, poetry_format, mask_token)\n","            all_preds_tmp.append({'0':[m2_final]})\n","\n","            pred_v_1 = beam_search(raw_sentence= '<START> ' + m1_final + ' <SEP> ',\n","                                    beam_depth=beam_depth,\n","                                    model=model,\n","                                    tokenizer=tokenizer, \n","                                    top_k=5,\n","                                    banned_ids = banned_ids,\n","                                    mask_token=mask_token,\n","                                    end_token='<SEP>', \n","                                    start_token='<START>')\n","            \n","            pred_v_2 = beam_search(raw_sentence= '<SEP> ' + m2_final + ' <END>',\n","                                    beam_depth = beam_depth,\n","                                    model=model,\n","                                    tokenizer=tokenizer, \n","                                    top_k=5,\n","                                    banned_ids = banned_ids,\n","                                    mask_token=mask_token,\n","                                    end_token='<END>',\n","                                    start_token='<SEP>')\n","            \n","            #print('\\npred_v_1', pred_v_1)\n","            #print('pred_v_2', pred_v_2)\n","            all_preds.append(pred_v_1 + ' / ' + pred_v_2)\n","                #all_preds.append()\n","                \n","\n","                \n","\n","\n","\n","        df_output_beit['heuristics'].append([m[0].replace(mask_token, '_') for m in [list(n.values())[0] for n in all_preds_tmp]])\n","        \n","        df_output_beit['final_predicted_verses'].append(all_preds)\n","\n","        print('\\n')\n","        print(colored('heuristics: ', 'blue'), df_output_beit['heuristics'][i])\n","        print(colored('all_preds: ', 'green'), df_output_beit['final_predicted_verses'][i])\n","\n","\n","\n","    # cleaning the final file\n","\n","    df_output_beit = pd.DataFrame(df_output_beit)\n","    df_output_beit.loc[:, 'final_predicted_verses'] = df_output_beit.loc[:, 'final_predicted_verses'].apply(lambda x : [clean_fpv(i) for i in x])\n","\n","\n","    df_output_beit.loc[:, 'poetry_generated_MHA'] = df_output_beit.loc[:, 'poetry_generated_MHA'].apply(lambda x: clean_mha(x))\n","\n","    \n","    df_output_beit.to_csv(address, index=False)\n","\n","    return df_output_beit, address\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXSxD3Sl9Nh1"},"source":["# beam search example after importing the model\n","\n","beam_search(\n","    raw_sentence = f'دامانها {tokenizer.mask_token} {tokenizer.mask_token} فشاند جانها {tokenizer.mask_token} {tokenizer.mask_token} {tokenizer.mask_token} {tokenizer.mask_token} {tokenizer.mask_token} <SEP> <END>',\n","    beam_depth=2,\n","    model=model,\n","    tokenizer= tokenizer,\n","    top_k=5,\n","    banned_ids = banned_ids, mask_token = tokenizer.mask_token\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2CIgzNQnbhj"},"source":["models = ['BERT V2', 'BERT V3', 'Albert', 'Distilbert', 'Roberta']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-zdWM9XtLEYK"},"source":["# Distilbert\n"]},{"cell_type":"code","metadata":{"id":"84-UjNuamtGD"},"source":["mask_token, model, tokenizer, Roberta = import_model(model_name='Distilbert', on='beit', is_roberta = False)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVlAidCUQuW8"},"source":["out_, address = fill_masks_on_beit(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            numOfKeywords=20,\n","            mesra_first_elements_num=4, \n","            mesra_second_elements_num=4, banned_ids = banned_ids,\n","            m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'Distilbert_gh',\n","            output_folder = '.../Results/Experiment 2/DistilBERT/'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8O-Bmx3shXa9"},"source":["out_ # 20 key num"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcSkZvQ3owYz"},"source":["mask_token, model, tokenizer, Roberta = import_model(model_name='Distilbert', on='verse', is_roberta = False)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens)\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZH9z-n8xqwU"},"source":["out_d_v, address_d_v = fill_masks_on_verse(\n","    model=model, tokenizer=tokenizer, \n","    translator_output=translator_output,\n","    numOfKeywords=20,\n","    mesra_first_elements_num=4, \n","    mesra_second_elements_num=4, banned_ids = banned_ids,\n","    m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'DistilBERT_gh',\n","    output_folder = '.../Results/Experiment 2/DistilBERT')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fuh8jTowjKQU"},"source":["out_d_v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NpF6JBR1fLXL"},"source":["address_d_v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q0Bmpf5tfLSU"},"source":["out_d_v.to_csv('.../Results/Experiment 2/DistilBERT/Final_DistilBERT_gh_Verse_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKvDioo7LbMI"},"source":["# BERT V2"]},{"cell_type":"code","metadata":{"id":"csYOFl-umu8r"},"source":["mask_token, model, tokenizer, Roberta = import_model(model_name='BERT V2', on='beit', is_roberta = False)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPhEsYFmxquK"},"source":["out__b_b, address_b_b = fill_masks_on_beit(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            numOfKeywords=20,\n","            mesra_first_elements_num=4, \n","            mesra_second_elements_num=4, banned_ids = banned_ids,\n","            m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'BERT_V2_gh',\n","            output_folder = '.../Results/Experiment 2/Bert V2/'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VITIYluKxqr3"},"source":["out_b.to_csv('.../Results/Experiment 2/Bert V2/Final_BERT_V2_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_100_first.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMTKP3dpXNMW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIufI1KjZeDX"},"source":["# BERT V3"]},{"cell_type":"code","metadata":{"id":"tCVdppm-mvtr"},"source":["mask_token, model, tokenizer, Roberta = import_model(model_name='BERT V3', on='beit', is_roberta = False)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKxYaEZfZg6k"},"source":["out_b_3_b, address_b_3_b = fill_masks_on_beit(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            numOfKeywords=20,\n","            mesra_first_elements_num=4, \n","            mesra_second_elements_num=4, banned_ids = banned_ids,\n","            m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'BERT_V3_gh',\n","            output_folder = '.../Results/Experiment 2/Bert V3/'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOOjsO21g6tP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfsQdHXYZg4l"},"source":["mask_token, model, tokenizer, Roberta = import_model(model_name='BERT V3', on='verse', is_roberta = False)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens)\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REkYwCcIXNJ_"},"source":["out_b_v, address_b_v = fill_masks_on_verse(\n","    model=model, tokenizer=tokenizer, \n","    translator_output=translator_output,\n","    numOfKeywords=20,\n","    mesra_first_elements_num=4, \n","    mesra_second_elements_num=4, banned_ids = banned_ids,\n","    m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'BERT_V3_gh_last',\n","    output_folder = '.../Results/Experiment 2/Bert V3/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KG1j8hX2glas"},"source":["address_b_v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYN9qd7pXNHm"},"source":["out_b_v.to_csv(address_b_v, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7fZIibZxqn7"},"source":["gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJRRJoOES4VQ"},"source":["# Roberta"]},{"cell_type":"code","metadata":{"id":"mJj307Q5mwvL"},"source":["mask_token, model, tokenizer, is_roberta = import_model(model_name='Roberta', on='beit', is_roberta = True)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=is_roberta, all_unw_tokens=all_unw_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Fctr0PzLDTK"},"source":["out_r_v, address_r_b = fill_masks_on_beit(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            numOfKeywords=20,\n","            mesra_first_elements_num=4, \n","            mesra_second_elements_num=4, banned_ids = banned_ids,\n","            m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'Roberta_gh',\n","            output_folder = '.../Results/Experiment 2/Roberta/'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4LM1gwNLDNC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKVcpN7BLDRK"},"source":["mask_token, model, tokenizer, is_roberta = import_model(model_name='Roberta', on='verse', is_roberta = True)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=is_roberta, all_unw_tokens=all_unw_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vO-XgjoHW6lB"},"source":["out_r_v, address_r_v = fill_masks_on_verse(\n","    model=model, tokenizer=tokenizer, \n","    translator_output=translator_output,\n","    numOfKeywords=20,\n","    mesra_first_elements_num=4, \n","    mesra_second_elements_num=4, banned_ids = banned_ids,\n","    m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'Roberta_gh',\n","    output_folder = '.../Results/Experiment 2/Roberta/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"foyHgN4UW6ja"},"source":["out_r_v.to_csv(address_r_v, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wM4qspoW6gm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E35t2egSW6d-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1djqRbwXLDPd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9LO_10edM03"},"source":["# Albert\n"]},{"cell_type":"code","metadata":{"id":"UcCNt_zumx0T"},"source":["mask_token, model, tokenizer, Roberta = import_model(model_name='Albert', on='beit', is_roberta = False)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ecKxQPALDK_"},"source":["out_a, address = fill_masks_on_beit(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            numOfKeywords=20,\n","            mesra_first_elements_num=4, \n","            mesra_second_elements_num=4, banned_ids = banned_ids,\n","            m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'Albert_gh',\n","            output_folder = '.../Results/Experiment 2/Albert/'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHPN3w2wLDI7"},"source":["out_a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"421U6pPYLDE6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8nnQRsuN8KB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RY0wp5FvN8Hp"},"source":["mask_token, model, tokenizer, Roberta = import_model(model_name='Albert', on='verse', is_roberta = False)\n","banned_ids = get_banned_tokens(model, tokenizer, is_roberta=False, all_unw_tokens=all_unw_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTvLCGz1LDC7"},"source":["out_a_v, address = fill_masks_on_verse(\n","    model=model, tokenizer=tokenizer, \n","    translator_output=translator_output,\n","    numOfKeywords=20,\n","    mesra_first_elements_num=4,  banned_ids = banned_ids,\n","    mesra_second_elements_num=4,\n","    m = 100, poetry_format = 'M', beam_depth = 3, model_name = 'Albert_gh',\n","    output_folder = '.../Results/Experiment 2/Albert/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RH6fqTnwLCyc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-h7E3SELCwV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2IoKrqSZNSO"},"source":["# *Other experiments*"]},{"cell_type":"code","metadata":{"id":"YSZhVkQYQuU2"},"source":["m = 10\n","poetry_format = 'M'\n","beam_depth = 2\n","mask_token=tokenizer.mask_token"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"md2LaC6OQuQ7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPdkYs9vQuO7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9B9ihd1QuMk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUPgJHCN_ePP"},"source":["m = 10\n","\n","mask_token = tokenizer.mask_token\n","df_output_beit = {\n","    'poetry_ground_truth': poetry_gt[:m],\n","    'poetry_generated_MHA': poetry_mha[:m],\n","    'final_predicted_verses' : [],\n","    'heuristics': []\n","    }\n","\n","for i in tqdm(range(0, len(poetry_mha[:m])), position=0, leave=True):\n","    \n","    \n","    # first get the heuristic masked seqs\n","    masked_sequences = heuristic(poetry_mha[i], mask_token)\n","\n","    \n","\n","\n","# FEDIT\n","    all_preds, all_preds_tmp = [], []\n","    poetry_format = 'GH'\n","    beam_depth = 2\n","\n","    \n","    for j in range(int(len(masked_sequences) / 2)):\n","        \n","        j *= 2\n","        m1, m2 = masked_sequences[j], masked_sequences[j+1]\n","\n","\n","        m1_final = final_sequence(all_preds_tmp, m1, j, poetry_format, mask_token)\n","        all_preds_tmp.append({'0':[m1_final]})\n","        \n","\n","        m2_final = final_sequence(all_preds_tmp, m2, j + 1, poetry_format, mask_token)\n","        all_preds_tmp.append({'0':[m2_final]})\n","\n","        #print(m1_final)\n","        \n","        all_preds.append(beam_search('<START> ' + m1_final + ' / ' + m2_final + ' <END>', \n","                                     beam_depth, model=model, tokenizer=tokenizer, top_k=5,\n","                                     banned_ids = banned_ids, mask_token=mask_token))\n","                \n","\n","\n","\n","\n","            \n","\n","    df_output_beit['heuristics'].append([m[0].replace(mask_token, '_') for m in [list(n.values())[0] for n in all_preds_tmp]])\n","    \n","    df_output_beit['final_predicted_verses'].append(all_preds)\n","\n","    print('\\n')\n","    print(colored('heuristics: ', 'blue'), df_output_beit['heuristics'][i])\n","    print(colored('all_preds: ', 'green'), df_output_beit['final_predicted_verses'][i])\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qH69VhnUEIly"},"source":["df_output_beit = pd.DataFrame(df_output_beit)\n","df_output_beit.loc[:, 'final_predicted_verses'] = df_output_beit.loc[:, 'final_predicted_verses'].apply(lambda x : [clean_fpv(i) for i in x])\n","\n","def clean_mha(txt):\n","\n","    txt = re.sub('<sep>', ' / ', txt)\n","    txt = re.sub('(<start>)|(<end>)', '', txt)\n","\n","    return txt\n","\n","df_output_beit.loc[:, 'poetry_generated_MHA'] = df_output_beit.loc[:, 'poetry_generated_MHA'].apply(lambda x: clean_mha(x))\n","\n","#df_output.loc[:, 'heuristics'] = df_output.loc[:, 'heuristics'].apply(lambda x: [i[0].replace(mask_token, '_') for i in x])\n","df_output_beit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lh3XKlcEVToh"},"source":["df_output_beit.to_csv('df_output_beit.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EmZqEVtVTmw"},"source":["df_output_beit.to_csv('.../Results/df_output_beit.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3zclEk8OUnp"},"source":["m = 10\n","\n","\n","df_output_beit = {\n","    'poetry_ground_truth': poetry_gt[:m],\n","    'poetry_generated_MHA': poetry_mha[:m],\n","    'final_predicted_verses' : [],\n","    'heuristics': []\n","    }\n","\n","heuristics = []\n","\n","for i in tqdm(range(0, len(poetry_mha[:m])), position=0, leave=True):\n","    \n","    \n","    # first get the heuristic masked seqs\n","    masked_sequences = heuristic(poetry_mha[i])\n","\n","    \n","\n","\n","# FEDIT\n","    all_preds, all_preds_tmp = [], []\n","    poetry_format = 'M'\n","    beam_depth = 2\n","\n","    \n","    for j in range(int(len(masked_sequences) / 2)):\n","        \n","        j *= 2\n","        m1, m2 = masked_sequences[j], masked_sequences[j+1]\n","\n","\n","        try:\n","            \n","            m1_final = final_sequence(all_preds_tmp, m1, j, poetry_format)\n","            all_preds_tmp.append({'0':[m1_final]})\n","\n","            m2_final = final_sequence(all_preds_tmp, m2, j + 1, poetry_format)\n","            all_preds_tmp.append({'0':[m2_final]})\n","\n","\n","            \n","            all_preds.append(beam_search('<START> ' + m1_final + ' / ' + m2_final + ' <END>', beam_depth))\n","                \n","\n","\n","\n","        except: \n","            print(colored('\\nEXCEPTION!!!', 'red'))\n","\n","            print([ ' '.join(i[0]) for i in masked_sequences])\n","            \n","            all_preds.append('Not a sequence')\n","            df_output_beit['heuristics'].append(m1_final + ' / ' + m2_final)\n","            \n","\n","            \n","\n","    df_output_beit['heuristics'].append([m[0].replace(mask_token, '_') for m in [list(n.values())[0] for n in all_preds_tmp]])\n","    \n","    df_output_beit['final_predicted_verses'].append(all_preds)\n","\n","    print('\\n')\n","    print(colored('heuristics: ', 'blue'), df_output_beit['heuristics'][i])\n","    print(colored('all_preds: ', 'green'), df_output_beit['final_predicted_verses'][i])\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggNbLvZwZPC4"},"source":["## CONT"]},{"cell_type":"code","metadata":{"id":"_WzuKk6DVTkd"},"source":["def fill_masks(model, tokenizer, translator_output, output_folder,\n","               numOfKeywords, mesra_first_elements_num, \n","                mesra_second_elements_num,\n","               m = 10, poetry_format = 'M', beam_depth = 2):\n","    \n","    \n","    \"\"\"\n","    create the output as a csv file with the\n","    ground truth\n","    poetry generated by the multiheadattention translator\n","    poetry generated after choosing keywords and filling the masks\n","    the output of heuristics\n","    \"\"\"\n","\n","\n","    # define the output format and address\n","\n","    address = output_address +f'final_{poetry_format}_poetry_format_{numOfKeywords}_numOfKeywords_{mesra_first_elements_num}_mesra_first_elements_num_{mesra_second_elements_num}_mesra_second_elements_num.csv'\n","\n","    \n","\n","    df_output_beit = {\n","        'poetry_ground_truth': poetry_gt[:m+1],\n","        'poetry_generated_MHA': poetry_mha[:m+1],\n","        'final_predicted_verses' : [],\n","        'heuristics': []\n","        }\n","\n","    # go through the dataset\n","\n","    mask_token = tokenizer.mask_token\n","\n","    for i in tqdm(range(0, len(poetry_mha[:m+1])), position=0, leave=True):\n","        \n","        # first get the heuristic masked seqs\n","        masked_sequences = heuristic(poetry_mha[i], mask_token, \n","                                     numOfKeywords, mesra_first_elements_num, \n","                                     mesra_second_elements_num)\n","        print(colored(i, 'green'))\n","\n","\n","        # processing each verse and them giving the whole\n","        # beit to transformers and beamsearch\n","        all_preds, all_preds_tmp = [], []\n","        #print(masked_sequences)\n","\n","        for j in range(int(len(masked_sequences) / 2)):\n","        \n","            j *= 2\n","\n","            m1, m2 = masked_sequences[j], masked_sequences[j+1]\n","\n","            print('\\nBefore')\n","            print('m1: ', m1[0])\n","            print('m2: ', m2[0])\n","            m1[0] = m1[0][:-3]\n","            m2[0] = m2[0][:-3]\n","            print('\\nAfter')\n","            print('m1: ', m1[0])\n","            print('m2: ', m2[0])\n","\n","            try:\n","                # verse 1\n","                m1_final = final_sequence(all_preds_tmp, m1, j, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m1_final]})\n","\n","                # verse 2\n","                m2_final = final_sequence(all_preds_tmp, m2, j + 1, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m2_final]})\n","\n","\n","                \n","                all_preds.append(beam_search('<START> ' + m1_final + ' <SEP> ' + m2_final + ' <END>', \n","                                            beam_depth, model=model, tokenizer=tokenizer, top_k=5,\n","                                            banned_ids = banned_ids, mask_token=mask_token))\n","\n","                \n","\n","            except: \n","\n","                m1_final = final_sequence(all_preds_tmp, m1, j, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m1_final]})\n","\n","                m2_final = final_sequence(all_preds_tmp, m2, j + 1, poetry_format, mask_token)\n","                all_preds_tmp.append({'0':[m2_final]})\n","\n","                print(colored('\\nEXCEPTION!!!', 'red'))\n","\n","                print([ ' '.join(i[0]) for i in masked_sequences])\n","                \n","                all_preds.append('Not a sequence')\n","                df_output_beit['heuristics'].append(m1_final + ' / ' + m2_final)\n","            \n","            \n","\n","        df_output_beit['heuristics'].append([m[0].replace(mask_token, '_') for m in [list(n.values())[0] for n in all_preds_tmp]])\n","        \n","        df_output_beit['final_predicted_verses'].append(all_preds)\n","\n","        print('\\n')\n","        print(colored('heuristics: ', 'blue'), df_output_beit['heuristics'][i])\n","        print(colored('all_preds: ', 'green'), df_output_beit['final_predicted_verses'][i])\n","\n","\n","\n","        if i%10==0:\n","            \n","            df_output_beit_ = {\n","        'poetry_ground_truth': poetry_gt[:i+1],\n","        'poetry_generated_MHA': poetry_mha[:i+1],\n","        'final_predicted_verses' : df_output_beit['final_predicted_verses'],\n","        'heuristics': df_output_beit['heuristics']\n","        }\n","            df_output_beit_ = pd.DataFrame(df_output_beit_)\n","            #df_output_beit_.to_csv(address, index=False)\n","\n","    df_output_beit_ = pd.DataFrame(df_output_beit)\n","    # cleaning the final file\n","    \n","    df_output_beit_.loc[:, 'final_predicted_verses'] = df_output_beit_.loc[:, 'final_predicted_verses'].apply(lambda x : [clean_fpv(i) for i in x])\n","\n","    df_output_beit_.loc[:, 'poetry_generated_MHA'] = df_output_beit_.loc[:, 'poetry_generated_MHA'].apply(lambda x: clean_mha(x))\n","\n","    df_output_beit_.to_csv(address, index=False)\n","\n","    return df_output_beit_\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OfdXT-D1PFR4"},"source":["fill_masks(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            output_folder = output_folder,\n","            numOfKeywords=6,\n","            mesra_first_elements_num=3, \n","            mesra_second_elements_num=3,\n","            m = 2, poetry_format = 'M', beam_depth = 3) # -2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kk9pzqUucCJZ"},"source":["fill_masks(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            output_folder = output_folder,\n","            numOfKeywords=6,\n","            mesra_first_elements_num=3, \n","            mesra_second_elements_num=3,\n","            m = 10, poetry_format = 'M', beam_depth = 3) # -1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awDoKtF6LvdX"},"source":["fill_masks(model=model, tokenizer=tokenizer, \n","            translator_output=translator_output,\n","            output_folder = output_folder,\n","            numOfKeywords=6,\n","            mesra_first_elements_num=3, \n","            mesra_second_elements_num=3,\n","            m = 2, poetry_format = 'M', beam_depth = 3) # -2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDH_JpdPLvbV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pwICYvaLvY4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGrkS76RQgoN"},"source":["for t in rhymes:\n","    for i in t:\n","        if i == 'بود':\n","            print(t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Lq1ZrOyV4LT"},"source":["rhymes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9K82fYrsV4JY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1o9amUmfV4HY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ODRj6vUV4FC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-0Iqa3bV4DD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NdrQ1t-V4A6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBYu6yC9ZSyR"},"source":["len(translator_output.loc[:, 'poetry_generated_MHA'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxEPXpN7ZYBH"},"source":["len(df_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLXE-wPKXijA"},"source":["df_output.to_csv('.../Results/final_output_15epochs_10May_BeitBert_0.7_6epochs_augmented_P&T.csv',\n","                 index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtpPxS1jbh-6"},"source":["df_output.to_csv('.../Results/final_output_15epochs_9May_BeitBert_0.7_6epochs_augmented_P&T.csv',\n","                 index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIzCFvJhHJTJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oI4hDA1CHJQO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYVuWdiY43m0"},"source":["df_output =  pd.DataFrame({'ground_truth': poetry_gt[:154],\n","             'poetry_mha':poetry_mha[:154],\n","             'final_predicted_verses': df_output['final_predicted_verses'][:154],\n","             'heuristics': df_output['heuristics'][:154],\n","             }\n","\n","             )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hj862mhVNZWm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqV7_bW-dPhE"},"source":["len(df_output['final_predicted_verses'][:154])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SBXmUbDdIQE"},"source":["len(df_output['heuristics'][:154])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EosnT4z-2gFC"},"source":["def beam_search(raw_sentence, mask_suggestions, beam_depth):\n","    leadings = [[['<START>'], 0]]\n","    mask_counter = 0\n","    for i, step in enumerate(raw_sentence.split()[1:]):\n","        print('STEP', step)\n","        if step == mask_token:\n","            all_candidates = []\n","            for lead in leadings:\n","                lead_thread, lead_score = lead\n","                print('BERT INPUT', ' '.join(lead_thread + raw_sentence.split()[i + 1:-1]))\n","                candidates = mask_suggestions[mask_counter]\n","                for j, cand in enumerate(candidates):\n","                    new_thread = lead_thread[-3:] + [cand]\n","                    # new_score = beam_scorer(new_thread, poetryBeam)\n","                    new_score = j\n","                    all_candidates.append([lead_thread + [cand], new_score])\n","                    all_candidates = sorted(all_candidates, key=lambda x:x[1], reverse=True)\n","                    print(new_thread, new_score)\n","            print('old leadings', leadings)\n","            leadings = all_candidates[:beam_depth]\n","            print('all_candidates', all_candidates)\n","            print('new leadings', leadings)\n","            mask_counter += 1\n","        elif step == '<END>':\n","            new_score = 1\n","            print('old leadings', leadings)\n","            for i in range(len(leadings)):\n","                # new_score = beam_scorer(each_lead[-3:] + ['<END>'], poetryBeam)\n","                new_score = 1\n","                leadings[i][1] += new_score\n","                leadings = sorted(leadings, key=lambda x:x[1], reverse=True)\n","            print('new leadings', leadings)\n","        else:\n","            # append step to all leading threads\n","            for each_lead in leadings:\n","                each_lead[0].append(step)\n","    return ' '.join(leadings[0][0])\n","\n","\n","raw_sentence = '<START> hello my [MASK] how [MASK] <END>'\n","mask_suggestions = [\n","                    ['friend', 'wife', 'husband'],\n","                    ['he', 'she', 'Ali']\n","]\n","beam_depth = 2\n","print(beam_search(raw_sentence, mask_suggestions, beam_depth))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lN7nBSJdHCYl"},"source":["\n","\n","# example\n","\n","get_transformer_suggestions('سلام بر تو ای [MASK] خوب [MASK]',\n","                            model= model_bert, \n","                            tokenizer=tokenizer_bert, \n","                            top_k=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VurAmTOhHJf8"},"source":["def mask_seq(sequence, model, tokenizer=tokenizer_bert):\n","    if tokenizer.mask_token == '<mask>':\n","        return re.sub(tokenizer.mask_token, '<mask>', sequence)\n","    else: return sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzIRmgj-HJb0"},"source":["def predict_seqs_dict(sequence, model, tokenizer, top_k=5, order='right-to-left'):\n","\n","    sequence = mask_seq(sequence, model, tokenizer)\n","\n","    ids_main = tokenizer.encode(sequence,\n","                            return_tensors=\"pt\",\n","                            add_special_tokens=False)\n","\n","    ids_ = ids_main.detach().clone()\n","    position = torch.where(ids_main == tokenizer.mask_token_id)\n","\n","    positions_list = position[1].numpy().tolist()\n","\n","    if order =='left-to-right':\n","        positions_list.reverse()\n","\n","    elif order=='random':\n","        random.shuffle(positions_list)\n","\n","    # print(positions_list)\n","    predictions_ids = {}\n","    predictions_detokenized_sents = {}\n","\n","    for i in range(len(positions_list)):\n","        predictions_ids[i] = []\n","        predictions_detokenized_sents[i] = []\n","        # where do we have mask?\n","        # print(i)\n","\n","\n","        \n","        # if it was the first prediction, \n","        # just go on and predict the first predictions\n","        \n","\n","        if i==0:\n","            model_logits = model(ids_main)['logits'][0][positions_list[0]]\n","            top_k_tokens = torch.topk(model_logits, top_k, dim=0).indices.tolist()\n","            \n","            for j in range(len(top_k_tokens)):\n","                #print(j)\n","                ids_t_ = ids_.detach().clone()\n","                ids_t_[0][positions_list[0]] = top_k_tokens[j]\n","                predictions_ids[i].append(ids_t_)\n","                \n","                pred = tokenizer.decode(ids_t_[0])\n","                predictions_detokenized_sents[i].append(pred)\n","\n","                # append the sentences and ids of this masked\n","                # token\n","\n","\n","        # if we already have some predictions, go on and fill the rest\n","        # of the masks by continuing the previous predictions\n","        if i!=0:\n","            for pred_ids in predictions_ids[i-1]:\n","                \n","                # get the logits\n","                #print('previous pred:', pred_ids)\n","                model_logits = model(pred_ids)['logits'][0][positions_list[i]]\n","                # get the top 5 of this prediction and masked token\n","                top_k_tokens = torch.topk(model_logits, top_k, dim=0)\\\n","                .indices.tolist()\n","\n","                for top_id in top_k_tokens:\n","                    # print(top_id)\n","                    \n","                    ids_t_i = pred_ids.detach().clone()\n","                    ids_t_i[0][positions_list[i]] = top_id\n","\n","                    #print(ids_t_i)\n","\n","                    #print(ids_)\n","                    pred = tokenizer.decode(ids_t_i[0])\n","                    #print(pred)\n","\n","                    # append the sentences and ids of this masked\n","                    # token\n","                    predictions_ids[i].append(ids_t_i)\n","                    predictions_detokenized_sents[i].append(pred)\n","                    \n","    return predictions_detokenized_sents\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SICkC1F4TQVs"},"source":["gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAOUAbQZTalK"},"source":["\n","\n","df_output = {\n","        'poetry_generated_MHA': poetry,\n","        'final_predicted_verses' : []\n","    }\n","\n","heuristics = []\n","for i in tqdm(range(0, len(poetry)), position=0, leave=True):\n","    \n","\n","    # first get the heuristic masked seqs\n","    masked_sequences = hueristic(poetry[i])\n","    heuristics.append(masked_sequences)\n","\n","\n","    predicted_verses = []\n","    all_preds = []\n","\n","    poetry_format = 'M'\n","\n","    # then for every verse loop and fill the blanks\n","    for j, each in enumerate(masked_sequences):\n","        \n","        filled_verse = predict_seqs_dict(\n","            final_sequence(\n","                all_preds, each, j, poetry_format\n","                ),\n","            model=model_bert,\n","            tokenizer=tokenizer_bert, \n","            top_k=1,\n","            order='right-to-left')\n","        \n","        all_preds.append(filled_verse)\n","\n","        \n","    all_verses = [list(l.values())[-1][0] for l in all_preds]\n","\n","    for m in range(2, len(all_verses)+1, 2):\n","        predicted_verses.append(' '.join(all_verses[m-2:m]))\n","        \n","    df_output['final_predicted_verses'].append(predicted_verses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BMEg8E7mOUqB"},"source":["\n","\n","df_output = {\n","        'poetry_generated_MHA': poetry,\n","        'final_predicted_verses' : []\n","    }\n","\n","heuristics = []\n","\n","for i in tqdm(range(0, len(poetry)), position=0, leave=True):\n","    \n","\n","    # first get the heuristic masked seqs\n","    masked_sequences = hueristic(poetry[i])\n","    heuristics.append(masked_sequences)\n","\n","\n","    predicted_verses = []\n","    all_preds, all_preds_tmp = [], []\n","    poetry_format = 'M'\n","\n","\n","\"\"\"\n","    # then for every verse loop and fill the blanks\n","    for j, each in enumerate(masked_sequences):\n","        \n","        filled_verse = predict_seqs_dict(\n","            final_sequence(\n","                all_preds, each, j, poetry_format\n","                ),\n","            model=model_bert,\n","            tokenizer=tokenizer_bert, \n","            top_k=1,\n","            order='right-to-left')\n","        \n","        all_preds.append(filled_verse)\n","\"\"\"\n","        \n","    all_verses = [list(l.values())[-1][0] for l in all_preds]\n","\n","    for m in range(2, len(all_verses)+1, 2):\n","        predicted_verses.append(' '.join(all_verses[m-2:m]))\n","        \n","    df_output['final_predicted_verses'].append(predicted_verses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ULVYiQw2BeN"},"source":["## motaradef"]},{"cell_type":"code","metadata":{"id":"toUCMJrVuZgq"},"source":["ganjoor_vocab = pd.read_pickle('.../Ganjoor_vocab.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yif-05R21BRX"},"source":["for i in range(len(motaradef['synonym'])):\n","\n","    unw = []\n","\n","    for j in range(len(motaradef.loc[i, 'synonym'])):\n","\n","        motaradef['synonym'][i][j] = motaradef['synonym'][i][j].strip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYwTOUEn2FL-"},"source":["motaradef"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PojDOrYk49at"},"source":["motaradef.to_csv('.../motaradef_9May.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IuZfDg45HiG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8TJVo-jruZV-"},"source":["for i in range(len(motaradef['synonym'])):\n","    unw = []\n","    for j in range(len(motaradef.loc[i, 'synonym'])):\n","\n","        if motaradef.loc[i, 'synonym'][j] not in ganjoor_vocab:\n","            unw.append(j)\n","            print(motaradef['synonym'][i][j])\n","\n","    for m in sorted(unw, reverse=True):\n","        del motaradef['synonym'][i][m]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UH4wN0bf2ZZ3"},"source":["motaradef#.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n25jumPV2fDv"},"source":["## Rhyme"]},{"cell_type":"code","metadata":{"id":"T55X5Yxh4EIW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZ_Mmsy8dZBz"},"source":["all_poems_beit_address = '.../beits_joined_cleaned.pickle'\n","rhymes_in_ganjour_address = '.../rhymes_in_ganjour.pickle'\n","#all_poems = pd.read_pickle(all_poems_beit_address)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhj8y4g-4NU5"},"source":["with open(rhymes_in_ganjour_address, 'wb') as f:\n","    pickle.dump(gh_g, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mo0lx0fHfJb3"},"source":["normalizer = hazm.Normalizer(persian_numbers=False)\n","\n","\n","def clean(txt):\n","\n","    txt = normalizer.normalize(txt)\n","\n","    txt = re.sub(r' \\s+', ' ', txt)\n","\n","    return txt\n","\n","for i in range(len(all_poems)):\n","    all_poems[i] = clean(all_poems[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlZzPnJ53tUO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAx5cV9mdY8g"},"source":["\n","\"\"\"\n","rhymes_in_ganjour = []\n","\n","for i in all_poems:\n","    words = i.split()\n","    first_g = words[-1]\n","    try: second_g = words[words.index('/')-1]\n","    except: print(i)\n","    print(first_g)\n","    print(second_g)\n","\n","    #rhymes_in_ganjour.append(first_g)\n","    #rhymes_in_ganjour.append(second_g)\n","    \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVl29xJvgu12"},"source":["#len(rhymes_in_ganjour)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SV_H0IuUeLD4"},"source":["#len(set(rhymes_in_ganjour))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cL4I4UwTg1kn"},"source":["rhymes_set = set(rhymes_in_ganjour)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOKjt6LIdJE4"},"source":["for n in range(len(rhymes)):\n","\n","    unw = []\n","\n","    for i in range(len(rhymes[n])):\n","        if rhymes[n][i] not in rhymes_set:\n","            unw.append(i)\n","            print(rhymes[n][i])\n","\n","    for j in sorted(unw, reverse=True):\n","        del rhymes[n][j]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4HgiaUNdMSy"},"source":["for i in rhymes:\n","    for j in i:\n","        if i =='لعنتی':\n","            print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mb3yiF57dMOn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXxhf2OhdMLv"},"source":["with open('.../rhymes_9May.pickle', 'wb') as f:\n","    pickle.dump(rhymes, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qa7ebHZddMJu"},"source":["rhymes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dH22WMYUdMHX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBJhY-L3A-RT"},"source":["unw = []\n","for i in range(len(motaradef['synonym'])):\n","    if motaradef['synonym'][i] == []:\n","        print(motaradef.loc[i, 'word'])\n","        unw.append(i)\n","\n","motaradef.drop(index = unw, inplace=True)\n","\n","motaradef.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNEGbHNGDD2y"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eb7fdspYKji1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmVoMA9BCJeq"},"source":["for i in motaradef['synonym']:\n","    if i == '':\n","        print('yea')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWwLMJsrH2mA"},"source":["for rhyme in rhymes :\n","    if len(rhyme)<3:\n","        print('yead')\n","        print(rhyme)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mR0xx9tssGAl"},"source":["Separating train test for verses\n"]},{"cell_type":"code","metadata":{"id":"uLl0o6uVsF5Y"},"source":["import glob\n","\n","!unzip \".../Ganjour/Ganjoor Dataset.zip\"\n","\n","# read all the poetry we have---------------------------------------\n","\n","all_poems = []\n","\n","for file_ in glob.glob('*.txt'):\n","    with open(file_) as f:\n","\n","        content = f.read()\n","        content = content.strip().split('\\n')\n","        content = [con for con in content if re.search('\\w', con)]\n","        all_poems = all_poems + content\n","\n","\n","print('Length of all poems poetries: ', \n","      colored(f\"{ len(all_poems):,}\", 'blue'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sUoibUo6eoP"},"source":["for i in all_poems:\n","    if re.search('سلام بر ', i):\n","        print('yead', i )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vyECdAtlwYMS"},"source":["gh_g = []\n","for m in range(len(poetry)):\n","    gh_g.append(poetry[m][0].split()[-1])\n","    gh_g.append(poetry[m][1].split()[-1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYbSRIBBxabq"},"source":["gh_g = set(gh_g)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNDSW1ojxi5x"},"source":["len(gh_g)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5Kto4cBsF3i"},"source":["poetry = []\n","for i in range(2, len(all_poems), 2):\n","    poetry.append((all_poems[i-2:i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rru-t6GbsF1a"},"source":["x_train, x_val = train_test_split(all_poems, shuffle = True,\n","                                                  test_size = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZAqn6nbsFwz"},"source":["val_path = '.../all_poetry_train_verse_10p_Val.csv'\n","train_path = '.../all_poetry_val_verse_90p_Train.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpi6jNl8tvti"},"source":["\n","pd.DataFrame(pd.Series(x_train), columns = ['poetry']).to_csv(train_path, index=False)\n","pd.DataFrame(pd.Series(x_val), columns = ['poetry']).to_csv(val_path, index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4-HcXC3sFzP"},"source":["from datasets import load_dataset\n","dataset_poetry = load_dataset('csv', data_files={'train': train_path,\n","                                                'test': val_path})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrQWP71gyXsO"},"source":["rhyme_old = pd.read_pickle('.../rhymes_list.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bD-1irByzHJ6"},"source":["def clean_rhyme(t):\n","    sub_1 = ['أ', ' مسلّم', 'محرّم', 'مقدّم', 'خرّم', 'ّ', 'نوّاب', 'مرکّب', 'محیّا', 'توّاب', 'دوّاب', 'مرکّب', 'محیّا']\n","    sub_2 = ['ا', ' مسلم', 'محرم', 'مقدم', 'خرم', '', 'نواب', 'مرکب', 'محیا', 'تواب', 'دواب', 'مرکب', 'محیا']\n","\n","    for i in range(len(sub_1)):\n","        t = re.sub(sub_1[i], sub_2[i], t)\n","    return t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRCmCF6fzhdx"},"source":["clean_rhymes = []\n","\n","for rs in rhyme_old:\n","    cl = []\n","    for r in rs:\n","        \n","        cl.append(clean_rhyme(r))\n","    clean_rhymes.append(cl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3wOseM30WxR"},"source":["clean_rhymes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe35m8XVyXqL"},"source":["unw = []\n","\n","for i in range(len(rhyme_old)):\n","    unw = []\n","    for j in range(len(rhyme_old[i])):\n","        if rhyme_old[i][j] not in gh_g:\n","            print(rhyme_old[i][j])\n","            unw.append(j)\n","\n","    for m in sorted(unw, reverse=True):\n","        del rhyme_old[i][m]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VydTJKdkyXoG"},"source":["unw = []\n","\n","for i in range(len(rhyme_old)):\n","    unw = []\n","    for j in range(len(rhyme_old[i])):\n","        if rhyme_old[i][j] not in gh_g:\n","            print(rhyme_old[i][j])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqDrid6PyXl5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9h-xE9D_yXjy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWReqidXyXhr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ni8OvwVYBBIr"},"source":["with open('.../rhymes_15May.pickle', 'wb') as f:\n","    pickle.dump(rhymes, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yK1AEnSW9fYi"},"source":["## For saving the model"]},{"cell_type":"code","metadata":{"id":"mGbOmDkn9Lh8"},"source":["tokenizer_bert = AutoTokenizer.from_pretrained(model_checkpoint_bert_V2,\n","                                               use_fast=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2m395Yz6Ei5"},"source":["tokenizer_bert = AutoTokenizer.from_pretrained(model_checkpoint_bert_V2,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ahOXns6598h"},"source":["tokenizer_bert.add_tokens(\n","    ['برآرد', 'برآید', 'وزآن', 'درآمد', \n","    'بدانگهی', 'نام‌آو',\n","    'ناآشنا', 'بدخویی', 'براندیشم']\n","                      )\n","\n","model_bert_V2_04_b = AutoModelForMaskedLM.from_pretrained(model_checkpoint_bert_V2)\n","\n","model_bert_V2_04_b.resize_token_embeddings(len(tokenizer_bert))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWg_vW8k6Ym9"},"source":["del G_ghafie, G, motaradef, rhymes,"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9yFWMKw4k-t"},"source":["model_folder_path_bert = r'.../Pretrained Models/BERT_V2_all_poems_mask_0.4/'\n","\n","\n","model_bert_V2_04_b.load_state_dict(torch.load('.../Pretrained Models/BERT_V2_all_poems_mask_0.4/BERT_V2_all_poems_mask_0.4_onCleanBeits.hpt', \n","                                              map_location='cpu'))\n","\n","#tokenizer_bert = AutoTokenizer.from_pretrained(model_checkpoint_bert, use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcJmRvzG4k7q"},"source":["model_bert_V2_04_b.save_pretrained(model_folder_path_bert)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UifSHIHU3GRs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mcnRx073GO-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1FoyqIy3GLy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zC7qJ-hY3GIs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLvI94aYYhF1"},"source":[""],"execution_count":null,"outputs":[]}]}