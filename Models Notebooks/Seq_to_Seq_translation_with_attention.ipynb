{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq_to_Seq_translation_with_attention.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KXhuDR4Tx60S"},"source":["# Packages"]},{"cell_type":"code","metadata":{"id":"pPH6Yu6fx3dP"},"source":["!pip install hazm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CVswsNR6yDK6"},"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import gc\n","import string\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pandas as pd\n","import hazm\n","pd.set_option('display.max_rows', 50)\n","pd.set_option('display.max_colwidth', None)\n","from termcolor import colored\n","from itertools import chain\n","#from transformers import BertTokenizer, BertModel\n","who_am_i = 'Mitra'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IHQJ8kr1yIRQ"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"XCB4yVh4yG6w"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iezEBwcByJkO"},"source":["all_data = pd.read_csv('.../ProsPoemParallelDataset_augmented.csv')\n","\n","print('length of augmented cleaned data: ', \n","      colored(len(all_data), 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgDZab2T9vSl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CuioUsQ9vPG"},"source":["val_indices = pd.read_pickle('.../validation_indices.pickle')\n","train_indices = pd.read_pickle('.../train_indices.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35B_eDiUHtBZ"},"source":["def clean(t):\n","    t = re.sub('^ ', '', t)\n","    t = re.sub(' $', '', t)\n","    t = re.sub(r' */ *', ' / ', t)\n","    t = t.replace('\\\\', '')\n","    t = re.sub(r' \\. *\\.', '\\.', t)\n","    t = re.sub(' +\\s', ' ', t)\n","\n","    t = re.sub(' \\.$', '\\.', t)\n","    t = re.sub('^ *\\. *', '', t)\n","\n","    t = re.sub('[۱۲۳۴۵۶۷۸۹۰]', '', t)\n","    \n","    return t\n","\n","all_data.loc[:, 'poetry'] = all_data.loc[:, 'poetry'].apply(lambda x: clean(x))\n","all_data.loc[:, 'text'] = all_data.loc[:, 'text'].apply(lambda x: clean(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"17w3_2PmzgPE"},"source":["all_data.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTCoVKX30GgR"},"source":["# PreProcessing + Creating Inputs"]},{"cell_type":"code","metadata":{"id":"ogKI5G_azk2B"},"source":["normalizer = hazm.Normalizer(persian_numbers=False)\n","\n","def process_sents(text):\n","    \n","    # separate dot or / from text with\n","    # one white space\n","    text = normalizer.normalize(text)\n","\n","    text = re.sub(r'([\\/\\.])', r' \\1', text)\n","\n","    # substitute / with sep between mesras\n","    text = re.sub(r' *\\/ *', ' <sep> ', text)\n","    \n","    # substitute any white space with one space\n","    text = re.sub(r'\\s+', ' ', text)\n","    \n","    # add start and end tokens\n","    text = '<start> ' + text + ' <end>'\n","    \n","    return text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMtvwQSs0JKG"},"source":["def tokenize(lang):\n","    # use keras defualt tokenizer\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        filters=''\n","    )\n","    # fit on the vocabulary used in text\n","    lang_tokenizer.fit_on_texts(lang)\n","\n","    # convert to ids\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                           padding = 'post')\n","    \n","    # add sep to the tokenizer\n","    #idx_sep = len(lang_tokenizer.index_word.keys())+1#[-1]\n","\n","    #lang_tokenizer.word_index['<sep>'] = idx_sep\n","    #lang_tokenizer.index_word[idx_sep] = '<sep>'\n","\n","\n","    return tensor, lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uh_PWvqN0Ymn"},"source":["def create_load_dataset(df):\n","\n","    input_lang = df.loc[:, 'text'].values.tolist()\n","    target_lang = df.loc[:, 'poetry'].values.tolist()\n","\n","    # preprocess each sentence\n","    input_lang = [process_sents(text) for text in input_lang]\n","    target_lang = [process_sents(text) for text in target_lang]\n","\n","    # create a tensor and tokenizer for each language\n","    input_tensor, input_lang_tokenizer = tokenize(input_lang)\n","    target_tensor, target_lang_tokenizer = tokenize(target_lang)\n","\n","    return input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUXdY3_o0J0I"},"source":["input_tensor, target_tensor,\\\n","input_lang_tokenizer, target_lang_tokenizer = create_load_dataset(all_data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUercjr_2uKJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yF8FjLTV0JyQ"},"source":["max_len_input = input_tensor.shape[1]\n","max_len_target = target_tensor.shape[1]\n","\n","print('longest sequence and the length of texts: ',\n","      colored(max_len_input, 'blue'))\n","print('longest sequence and the length of poetries: ',\n","      colored(max_len_target, 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hl7qZtXP4irR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XlXGwLTv1cl5"},"source":["# Vocabularies"]},{"cell_type":"code","metadata":{"id":"lPftSMTr1ZqY"},"source":["# lenght of constructed vocabularies:\n","# 1 for padding\n","vocab_len_i = len(input_lang_tokenizer.index_word) + 1\n","print(\"Plain text vocab has\", colored(f\"{vocab_len_i:,}\", 'green'), \"unique words.\")\n","\n","vocab_len_t = len(target_lang_tokenizer.index_word) + 1\n","print(f\"Poetry vocab has\", colored(f\"{vocab_len_t:,}\", 'green'), \"unique words.\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhj56O291e1x"},"source":["def convert(text, poetry):\n","\n","\n","    print(colored('Text:', 'green'))\n","    for i in text:\n","        if i!=0:\n","            print(\"%d -----> %s\"%(i, input_lang_tokenizer.index_word[i]))\n","        \n","    print(colored('\\nPoetry:', 'green'))\n","    for i in poetry:\n","        if i!=0:\n","            print(\"%d -----> %s\"%(i, target_lang_tokenizer.index_word[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92tVL-8g1vUY"},"source":["print(colored('Text: ', 'blue'), all_data.loc[5, 'text'])\n","print(colored('Poetry: ', 'blue'), all_data.loc[5, 'poetry'])\n","convert(input_tensor[5], target_tensor[5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBJAjAwD1IJa"},"source":["# Creating the Model"]},{"cell_type":"code","metadata":{"id":"x8jm6TxX5R6J"},"source":["input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val =\\\n","input_tensor[train_indices], input_tensor[val_indices],  target_tensor[train_indices], target_tensor[val_indices]\n","\n","print('Length of train and val:', \n","      colored(f\"{len(input_tensor_train), len(input_tensor_val)}\", 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7u8Iv0As0Jwa"},"source":["# defining the main parameters of the model\n","# and the inputs\n","\n","len_data = len(input_tensor_train)\n","batch_s = 64\n","steps_per_\n"," = len_data // batch_s\n","embedding_dim = 256\n","units = 1024"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVyXi7KO0JuQ"},"source":["# create the dataset and shuffle all\n","len_data_train = len(input_tensor_train)\n","len_data_test = len(target_tensor_val)\n","\n","# creat the datasets and put them in batches\n","\n","train_batches = tf.data.Dataset.from_tensor_slices((\n","    np.array(input_tensor_train.tolist(), dtype='int32'),\n","     np.array(target_tensor_train.tolist(), dtype='int32')\n",")).shuffle(len_data_train).batch(batch_s, drop_remainder=True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGQvHp6i1FPL"},"source":["input_batch_sample, target_batch_sample = next(iter(train_batches))\n","\n","print('A sample of text(input) batch: \\n', \n","      colored(input_batch_sample, 'blue'))\n","\n","print('\\nA sample of poetry(target) batch: \\n', \n","      colored(target_batch_sample, 'blue'))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdaWqqGe1FMo"},"source":["class Encoder(tf.keras.Model):\n","\n","    def __init__(self, vocab_size, embedding_dim,\n","                 enc_units, batch_s, ):\n","        \n","        # change the primary model from keras\n","        super(Encoder, self).__init__()\n","\n","        # define the parameters\n","        self.batch_s = batch_s\n","        self.enc_units = enc_units\n","        self.embeddings = tf.keras.layers.Embedding(vocab_size,\n","                                                    embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        \n","    \n","    def call(self, x, hidden):\n","        # first part of the model\n","        # calling the embeddings and giving them\n","        # to the gru\n","        x = self.embeddings(x)\n","        output, state = self.gru(x, initial_state=hidden)\n","        return output, state\n","\n","\n","    def initialize_hidden_state(self):\n","        # the initial state of the hidden states\n","        # start with zeros\n","        return tf.zeros((self.batch_s, self.enc_units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z11DHGRm1FKi"},"source":["encoder = Encoder(vocab_len_i, embedding_dim, units, batch_s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o8thv9s91FFo"},"source":["# initialize the hs\n","sample_hidden_states = encoder.initialize_hidden_state()\n","# get the output of the encoder\n","sample_encoder_output, sample_hidden_states_encoder = encoder(input_batch_sample, sample_hidden_states)\n","\n","print('Encoder hidden states shapes:',\n","      colored(sample_hidden_states_encoder.shape, 'blue'))\n","print('Encoder output shape:',\n","      colored(sample_encoder_output.shape, 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSuX60f41FDx"},"source":["print('Hidden states after being processed in gru:\\n',\n","      colored(sample_hidden_states_encoder, 'blue'))\n","\n","print('\\nEncoder output sample:\\n', colored(sample_encoder_output, 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTafjWk71E85"},"source":["class Decoder(tf.keras.Model):\n","    # create the decoder side\n","\n","    def __init__(self, vocab_size, embedding_dim, \n","                 decoder_units, batch_s, ):\n","        \n","        # take and change the keras.model\n","        super(Decoder, self).__init__()\n","        # parameters\n","        self.batch_s = batch_s\n","        self.decoder_units = decoder_units\n","        self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","\n","        self.gru = tf.keras.layers.GRU(self.decoder_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        # fully connected\n","        self.fc = tf.keras.layers.Dense(vocab_size, \n","                                        activation='softmax')\n","\n","        self.attention = BahdanauAttention(self.decoder_units)\n","\n","    def call(self, x, hidden, encoder_output):\n","        # construct the decoder\n","\n","        # x shape = (batch_s, 1)\n","        # one id for each word from the target\n","        \n","        # encoder output = (batch_s, max_len, hidden_states_s)\n","        context_vector = self.attention(query=hidden, \n","                                                           value=encoder_output)\n","        \n","        # we expand the ids into embedding vectors\n","        # x = (batch_s, 1, embedding_dim)\n","        x = self.embeddings(x)\n","\n","        # concatenating hidden states and the context\n","        # vector\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x],\n","                      axis=-1)\n","        \n","        # give both attention and embeddings to gru\n","        output, state = self.gru(x)\n","\n","        # output = (batch_size, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","\n","        # batch_s, vocab_size\n","        x = self.fc(output)\n","\n","        return x, state\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70UphWDE1E6h"},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    \n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, value):\n","        # query = batch_s, hidden_states - from decoder\n","        # query_with_time = batch_s, 1, hidden_states\n","        # values = batch_s, max_len_input, hidden_states\n","\n","        # adding one dimention to take time into account\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","\n","\n","        # combining values and queries\n","        # attention scores = batch_s, max_len, 1\n","        attention_score = self.V(tf.nn.tanh(\n","            self.W1(query_with_time_axis) + self.W2(value)\n","        ))\n","\n","        # getting a softmax to choose the weights for\n","        # each position in input\n","        # batch_s, max_len_input, 1\n","        attention_weights = tf.nn.softmax(attention_score, axis=1)\n","\n","        # after multiplication and summing:\n","        # context_vector = batch_s, hidden_s\n","        context_vector = attention_weights * value\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNM2yJQW1E4Q"},"source":["decoder = Decoder(vocab_len_t, embedding_dim, units, batch_s)\n","\n","\n","sample_decoder_output, states = decoder(x = tf.random.uniform((batch_s, 1)), \n","                                        hidden = sample_hidden_states_encoder,\n","                                        encoder_output = sample_encoder_output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NssDHW756Dwi"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"uiHTvQRC1E1x"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","def loss_function(real, pred):\n","\n","    # first mask the ones that are not paddings\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","\n","    # apply the loss on the whole sequence\n","    loss_ = loss_object(real, pred)\n","\n","    # make the mask datatype the same as loss\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","\n","    loss_ = loss_ * mask\n","\n","    # return the mean of all words\n","    return tf.reduce_mean(loss_)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d561K3pc0JsA"},"source":["\n","\n","# Switching from eager execution to graph execution\n","\n","@tf.function\n","def train_step(input, target, encoder_hidden):\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        # get to the encoder\n","        encoder_output, encoder_hidden_states = encoder(input, encoder_hidden)\n","\n","        # start the decoder\n","        decoder_hidden_states = encoder_hidden_states\n","\n","        # give <s> to all as the first word\n","        decoder_input = \\\n","        tf.expand_dims([target_lang_tokenizer.word_index['<start>']] * batch_s, 1)\n","\n","    # Teacher Forcing \n","    # start from the first word and continue\n","    # till the end of the sequence\n","\n","        for t in range(1, target.shape[1]):\n","\n","            # give \n","            # 1. decoder input that starts with <start>\n","            # 2. decoder hidden states\n","            # 3. encoder output\n","            # to the decoder\n","\n","            Seq2Seq_logits, decoder_hidden_states = decoder(\n","                decoder_input, decoder_hidden_states, encoder_output\n","            )\n","\n","            # ------------------------------------\n","            # calculate the loss at time step t\n","            loss += loss_function(target[:, t], Seq2Seq_logits)\n","\n","            # change the decoder input to the target token of \n","            # this time step for \n","            # Teacher Forcing\n","            decoder_input = tf.expand_dims(target[:, t], axis=1)\n","        \n","    # get the mean loss\n","    batch_loss = (loss / int(target.shape[1]))\n","\n","    # get the variables that have been changed\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    # calculate the gradients based on the loss\n","    gradients = tape.gradient(loss, variables)\n","\n","    optimizer.apply_gradients(zip(gradients, variables))\n","\n","    return batch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pn1-V2mn0Jp4"},"source":["gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIzew4sr6biA"},"source":["# alpha = 0.6\n","epochs = 13 \n","start_id = '<start>'\n","\n","for epoch in range(epochs):\n","\n","    print(\"epoch \", colored(epoch, 'blue'))\n","    start = time.time()\n","\n","    encoder_hidden_state = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (input, target)) in enumerate(train_batches.take(steps_per_epoch)):\n","\n","        batch_loss = train_step(input, target, encoder_hidden_state)\n","        total_loss += batch_loss\n","\n","        gc.collect()\n","        \n","        if batch%50==0:\n","            print('batch ', colored(batch, 'green'),\n","                  f' Loss {batch_loss.numpy():.4f}')\n","\n","    #checkpoint.save(file_prefix=checkpoint_prefix)\n","    print(f'Time taken: {time.time() - start:.2f} seconds')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDNK0ntHR2BC"},"source":["# Normal Evaluation"]},{"cell_type":"code","metadata":{"id":"d8DsisU06bf4"},"source":["def evaluate(sentence):\n","\n","\n","    # attention_plot = np.zeros((max_len_target,\n","      #                         max_len_input))\n","\n","    # preprocessing every sentence before giving\n","    # them to the model\n","    sentence = process_sents(sentence)\n","    # converting str to ids and padding and creating a tensor\n","    # from all\n","    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                          maxlen=max_len_input,\n","                                                          padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","\n","    # start constructing the output string\n","    output = ''\n","\n","    hidden_state = [tf.zeros((1, units))]\n","    encoder_output, encoder_hidden_state = encoder(inputs, hidden_state)\n","\n","    decoder_hidden_state = encoder_hidden_state\n","\n","    decoder_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n","\n","\n","    for t in range(max_len_target):\n","        predictions, decoder_hidden_state = decoder(\n","            decoder_input, decoder_hidden_state,\n","            encoder_output\n","        )\n","\n","\n","        # sotring for plot\n","        # attention_weights = tf.reshape(attention_weights, (-1, ))\n","        # attention_plot[t] = attention_weights.numpy()\n","        # predict the most probable token\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        \n","        # add this token to the previous ones\n","        output = output + target_lang_tokenizer.index_word[predicted_id] + ' '\n","\n","        # is it over?\n","        if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n","            return output, sentence\n","\n","        # give the prediction to continue predicting \n","        # next tokens\n","        decoder_input = tf.expand_dims([predicted_id], 0)\n","\n","\n","    return output, sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQpUMwo-6bd5"},"source":["def transform(text):\n","    output, text = evaluate(sentence=text)\n","\n","    print('Text: ', text)\n","    print('Generate poetry:', output)\n","\n","\n","    # attention_plot = attention_plot[:len(output.split(' ')),\n","     #                             :len(text.split(' '))]\n","    # plot_attention(attention_plot, text.split(' '), output.split(' '))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VAB3c-GU3xD"},"source":["transform('با این توصیف عشاق بی عقل و بدون هدف خاص زندگی می کنند و دارای هیچ هدف و مغزی نیستند تا اینکه به جهنم می رسند و به هیچ جایگاه دنیوی و واقعی دست پیدا نمی کنند')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"91GOZVAlBUk1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2MZMp_i_6u1"},"source":["# evaluate a dataset"]},{"cell_type":"code","metadata":{"id":"hO7UZz-sKXDD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JnCHQ2UKU6D"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UsFlIxyKT6b"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvzhFZzgU3sI"},"source":["def evaluate_dataset(df):\n","\n","    generated_p = []\n","\n","    df = df.reset_index(drop=True)\n","    for r in range(len(df)):\n","\n","        try:\n","            # attention_plot = np.zeros((max_len_target,\n","            #                         max_len_input))\n","\n","            # preprocessing every sentence before giving\n","            # them to the model\n","            sentence = process_sents(df.loc[r, 'text'])\n","            # converting str to ids and padding and creating a tensor\n","            # from all\n","            inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n","            inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                                maxlen=max_len_input,\n","                                                                padding='post')\n","            inputs = tf.convert_to_tensor(inputs)\n","\n","            # start constructing the output string\n","            output = ''\n","\n","            hidden_state = [tf.zeros((1, units))]\n","            encoder_output, encoder_hidden_state = encoder(inputs, hidden_state)\n","\n","            decoder_hidden_state = encoder_hidden_state\n","\n","            decoder_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n","\n","\n","            for t in range(max_len_target):\n","                predictions, decoder_hidden_state = decoder(\n","                    decoder_input, decoder_hidden_state,\n","                    encoder_output\n","                )\n","\n","\n","                # sotring for plot\n","                # attention_weights = tf.reshape(attention_weights, (-1, ))\n","                # attention_plot[t] = attention_weights.numpy()\n","                # predict the most probable token\n","                predicted_id = tf.argmax(predictions[0]).numpy()\n","                \n","                # add this token to the previous ones\n","                output = output + target_lang_tokenizer.index_word[predicted_id] + ' '\n","\n","                # is it over?\n","                if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n","                    break\n","\n","                # give the prediction to continue predicting \n","                # next tokens\n","                decoder_input = tf.expand_dims([predicted_id], 0)\n","\n","            generated_p.append(output)\n","\n","        except: \n","            print(r)\n","            print(df.loc[r, 'text'])\n","\n","            generated_p.append(None)\n","\n","    df_output = pd.concat([df, pd.Series(generated_p)],\n","                                axis = 1)\n","            \n","    df_output.columns = ['poetry_ground_truth',\n","                        'text',\n","                        'poetry_generated_Seq2Seq_with_Att']\n","\n","\n","    return df_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vk1FnbgbMDDj"},"source":["val_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4V6a1D_fCBg-"},"source":["\n","output_df = evaluate_dataset(all_data.loc[val_indices])\n","output_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQBNvR3iB4fV"},"source":["output_df.to_csv(f'.../Results/Phase|Models/Seq2Seq_with_att_{epochs}_epochs_{batch_s}_batch_s_{embedding_dim}_embedding_dim_{units}_units_.csv',\n","                 index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6UodNtFB4TB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDZLOgLQB4QO"},"source":[""],"execution_count":null,"outputs":[]}]}