{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recurrent Neural Network-with GRU.ipynb","provenance":[{"file_id":"1SV5__AlUtfvHJiR8IKGsAp50ssictQ-z","timestamp":1621326162759},{"file_id":"1ktav3Us0vdiUqbnhuWckFW-U7zry6N0a","timestamp":1621287209289}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PsKSZg49rE9o"},"source":["!pip install hazm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVRzcbrcoOrF"},"source":["import collections\n","\n","import helper\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","import string\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pandas as pd\n","import hazm\n","\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model, Sequential\n","from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy\n","import tensorflow as tf\n","pd.set_option('display.max_rows', 50)\n","pd.set_option('display.max_colwidth', None)\n","from termcolor import colored\n","from itertools import chain\n","#from transformers import BertTokenizer, BertModel\n","who_am_i = 'Mitra'\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ATx5fijocLE"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPQAg6EFocIK"},"source":["who_am_i = 'Mitra'\n","\n","\n","all_data = pd.read_csv('.../final/ProsPoemParallelDataset_augmented.csv')\n","\n","print('length of augmented cleaned data: ', \n","      colored(len(all_data), 'blue'))\n","val_indices = pd.read_pickle('.../validation_indices.pickle')\n","train_indices = pd.read_pickle('.../train_indices.pickle')\n","\n","\n","def clean(t):\n","    t = re.sub('^ ', '', t)\n","    t = re.sub(' $', '', t)\n","    t = re.sub(r' */ *', ' / ', t)\n","    t = t.replace('\\\\', '')\n","    t = re.sub(r' \\. *\\.', '\\.', t)\n","    t = re.sub(' +\\s', ' ', t)\n","\n","    t = re.sub(' \\.$', '\\.', t)\n","    t = re.sub('^ *\\. *', '', t)\n","\n","    t = re.sub('[۱۲۳۴۵۶۷۸۹۰]', '', t)\n","    \n","    return t\n","\n","all_data.loc[:, 'poetry'] = all_data.loc[:, 'poetry'].apply(lambda x: clean(x))\n","all_data.loc[:, 'text'] = all_data.loc[:, 'text'].apply(lambda x: clean(x))\n","\n","\n","all_data.reset_index(inplace=True, drop=True)\n","all_data.drop(columns=['Unnamed: 0'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hrVH4fuRZEi"},"source":["validation_set = all_data.loc[val_indices]\n","train_set = all_data.loc[train_indices]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJpsKJ8Xob86"},"source":["normalizer = hazm.Normalizer(persian_numbers=False)\n","\n","def process_sents(text):\n","    \n","    # separate dot or / from text with\n","    # one white space\n","    text = normalizer.normalize(text)\n","\n","    text = re.sub(r'([\\/\\.])', r' \\1', text)\n","\n","    # substitute / with sep between mesras\n","    text = re.sub(r' *\\/ *', ' <sep> ', text)\n","    \n","    # substitute any white space with one space\n","    text = re.sub(r'\\s+', ' ', text)\n","    \n","    # add start and end tokens\n","    text = '<start> ' + text + ' <end>'\n","    \n","    return text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7JY13ywuLno"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1k1TztbttQt4"},"source":["def tokenize(lang, target=True, max_len=35):\n","    # use keras defualt tokenizer\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        filters=''\n","    )\n","    # fit on the vocabulary used in text\n","    lang_tokenizer.fit_on_texts(lang)\n","\n","    # convert to ids\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                           padding = 'post',\n","                                                           maxlen=max_len)\n","        \n","\n","\n","    return tensor, lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lx9g_SA1ob6J"},"source":["def create_load_dataset(df):\n","\n","    input_lang = df.loc[:, 'text'].values.tolist()\n","    target_lang = df.loc[:, 'poetry'].values.tolist()\n","\n","    # preprocess each sentence\n","    input_lang = [process_sents(text) for text in input_lang]\n","    target_lang = [process_sents(text) for text in target_lang]\n","\n","    # create a tensor and tokenizer for each language\n","    input_tensor, input_lang_tokenizer = tokenize(input_lang)\n","    max_len_input = input_tensor.shape[1]\n","\n","    target_tensor, target_lang_tokenizer = tokenize(target_lang, target=True,\n","                                                    max_len=max_len_input)\n","\n","    return input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zySsnfFBob3i"},"source":["input_tensor, target_tensor,\\\n","input_lang_tokenizer, target_lang_tokenizer = create_load_dataset(all_data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQCZkqCSob0r"},"source":["max_len_input = input_tensor.shape[1]\n","max_len_target = target_tensor.shape[1]\n","\n","print('longest sequence and the length of texts: ',\n","      colored(max_len_input, 'blue'))\n","print('longest sequence and the length of poetries: ',\n","      colored(max_len_target, 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGF0dJACobxz"},"source":["# lenght of constructed vocabularies:\n","# 1 for padding\n","vocab_len_i = len(input_lang_tokenizer.index_word) + 1\n","print(\"Plain text vocab has\", colored(f\"{vocab_len_i:,}\", 'green'), \"unique words.\")\n","\n","vocab_len_t = len(target_lang_tokenizer.index_word) + 1\n","print(f\"Poetry vocab has\", colored(f\"{vocab_len_t:,}\", 'green'), \"unique words.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HFx0FRcobvB"},"source":["def convert(text, poetry):\n","\n","\n","    print(colored('Text:', 'green'))\n","    for i in text:\n","        if i!=0:\n","            print(\"%d -----> %s\"%(i, input_lang_tokenizer.index_word[i]))\n","        \n","    print(colored('\\nPoetry:', 'green'))\n","    for i in poetry:\n","        if i!=0:\n","            print(\"%d -----> %s\"%(i, target_lang_tokenizer.index_word[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gpbRX3qobsJ"},"source":["print(colored('Text: ', 'blue'), all_data.loc[5, 'text'])\n","print(colored('Poetry: ', 'blue'), all_data.loc[5, 'poetry'])\n","convert(input_tensor[5], target_tensor[5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tbNypWR91egG"},"source":["# Seq to seq model with embedding in the beginning"]},{"cell_type":"code","metadata":{"id":"TCJW95WRobmK"},"source":["\n","def seq_2_seq(input_vocab_s, output_vocab_s, embedding_dim,\n","              gru_d=256, drop_out = 0.5, l_rate=0.002):\n","\n","    model = keras.Sequential()\n","    \n","    model.add(layers.Embedding(input_dim=input_vocab_s, output_dim=embedding_dim))\n","\n","    # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n","    model.add(layers.GRU(gru_d, return_sequences=True))\n","\n","    model.add(layers.Dropout(rate=drop_out))\n","\n","    # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n","    model.add(layers.Dense(output_vocab_s, activation='softmax'))\n","\n","\n","    model.compile(\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        optimizer=tf.keras.optimizers.SGD(\n","    learning_rate=l_rate),\n","        metrics=[\"accuracy\"],\n",")\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-y74nvBX0Mm"},"source":["model = seq_2_seq(vocab_len_i, vocab_len_t, embedding_dim=1024,\n","              gru_d=1024, drop_out = 0.5, l_rate=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3RqrCBb1j_W"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBTQguN7xYDJ"},"source":["np.exp(1.34)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YmGrVIoJdrKj"},"source":["input_tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdrCha2DvOg5"},"source":["from sklearn.model_selection import KFold\n","\n","\n","epochs = 5\n","n_splits = 3\n","batch_s = 64\n","\n","for epoch in range(epochs):\n","\n","    print(colored(f\"Epoch {epoch+1}\", 'green', attrs=['bold', 'underline']))\n","\n","\n","    start = time.time()\n","    for i, (train_index, test_index) in enumerate(KFold(n_splits).split(pd.Series(input_tensor.tolist()))):\n","\n","        print(colored(f'fold {i+1}', 'green'))\n","\n","        \n","        # first defining the train and val based on kfold splits\n","        input_tensor_train, input_tensor_val = input_tensor[train_index], input_tensor[test_index]\n","        target_tensor_train, target_tensor_val = target_tensor[train_index], target_tensor[test_index]\n","\n","\n","\n","        model.fit(x = input_tensor_train,\n","          y = target_tensor_train,\n","          validation_data=(input_tensor_val,target_tensor_val ) ,  \n","          batch_size = batch_s)\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVGe2g8I1j8w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Bl0LdVEqr6n"},"source":["\n","def evaluate(sent, max_len=40):\n","\n","    # preprocessing every sentence before giving\n","    # them to the model\n","    sentence = process_sents(sent)\n","    print(sentence)\n","    # input tokenizer\n","    inputs = [input_lang_tokenizer.word_index[w] for w in sentence.split(' ')]\n","\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                          maxlen=max_len_input,\n","                                                          padding='post')\n","    \n","    inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n","\n","    pred = model.predict(inputs)\n","    print(pred)\n","    prediction = tf.argmax(pred, axis=1, output_type=tf.int32)\n","    #print('prediction:', prediction)\n","    #print(prediction[0].numpy())\n","    #print(len(prediction[0]))\n","    pred_ = prediction.numpy()[0]\n","    output = ''\n","    for m in range(len(pred_)):\n","        #print(i)\n","        if  pred_[m]== target_lang_tokenizer.word_index['<end>']:\n","            break\n","        elif  pred_[m]==0:\n","                    break\n","        elif pred_[m] == pred_[m-1]: continue\n","        else: output = output + ' ' + target_lang_tokenizer.index_word[pred_[m]]\n","        \n","    \n","    \n","\n","    return output\n","\n","def print_poetry(sent):\n","\n","    poetry = evaluate(sent)\n","    print(colored('Text: ', 'green'), sent)\n","    print(colored('Poetry: ', 'green'), poetry)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLjXSrKrib6G"},"source":["def evaluate_dataset(df):\n","\n","    generated_p = []\n","\n","    df = df.reset_index(drop=True)\n","    for r in range(len(df)):\n","\n","        # preprocessing every sentence before giving\n","        # them to the model\n","        sentence = process_sents(df.loc[r, 'text'])\n","        #print(sentence)\n","        # input tokenizer\n","        inputs = [input_lang_tokenizer.word_index[w] for w in sentence.split(' ')]\n","\n","        inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                            maxlen=max_len_input,\n","                                                            padding='post')\n","        \n","        inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n","\n","        pred = model.predict(inputs)\n","        \n","        prediction = tf.argmax(pred, axis=1, output_type=tf.int32)\n","        #print('prediction:', prediction)\n","        #print(prediction[0].numpy())\n","        #print(len(prediction[0]))\n","        #print(prediction)\n","        \n","        pred_ = prediction.numpy()[0]\n","        print(prediction)\n","        \n","        if r==0:print(pred_)\n","\n","        #print(len(pred_))\n","\n","        output = ''\n","\n","        for m in range(len(pred_)):\n","            #print(i)\n","            #if  pred_[m]== target_lang_tokenizer.word_index['<end>'] or pred_[m] == target_lang_tokenizer.word_index['<start>']:\n","                #break\n","            if  pred_[m]==0 :break\n","            if  pred_[m]== target_lang_tokenizer.word_index['<end>'] :\n","                break\n","            #elif pred_[m] == pred_[m-1]: continue\n","            #elif pred_[m] == pred_[m-2]: continue\n","            output = output + ' ' + target_lang_tokenizer.index_word[pred_[m]]\n","        \n","        generated_p.append(output)\n","        \n","        \n","    df_output = pd.concat([df, pd.Series(generated_p)], axis = 1)\n","\n","    df_output.columns = ['poetry_ground_truth',\n","                        'text',\n","                        'poetry_generated_Seq2Seq_GRU']\n","\n","\n","    return df_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xx2Dt27-i-SN"},"source":["df_output = evaluate_dataset(all_data.loc[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mbUsdRIi-I1"},"source":["df_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xSxVPE58NOvw"},"source":["df_output.to_csv(f'.../Results/Phase|Models/Seq2Seq_with_GRU_epochs_{batch_size}_batch_size_{drop_out_r}_drop_out_r_{embedding_dim}_embedding_dim_{depth}_depth_{l_rate}_l_rate.csv',\n","                 index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMghwz2Dg9TK"},"source":["sys_inp = 'عاشقان در این زندگی به دنبال جایگاه دنیوی نیستند. آن ها این دنیا را پست و بی مقدار و خار می دانند.'\n","\n","print_poetry(sys_inp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfi-KVh7qrwC"},"source":[""],"execution_count":null,"outputs":[]}]}