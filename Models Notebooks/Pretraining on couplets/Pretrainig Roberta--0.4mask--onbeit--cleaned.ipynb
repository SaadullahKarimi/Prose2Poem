{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pretrainig Roberta--0.4mask--onbeit--cleaned.ipynb","provenance":[{"file_id":"1lqUNWyrlul8MEpubwHr0ouI8WYwephLk","timestamp":1620223467902},{"file_id":"14ULMHrN3BTjI9w-sT_QFKHOs0HlpRBCT","timestamp":1620115289180},{"file_id":"1gxR-mFjOxEI3hGvaMdddO87_hNCyrBR4","timestamp":1619791942785}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fPeuh14MO3yj"},"source":["!pip install datasets transformers\n","!pip install hazm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HrioHhGPiqJ"},"source":["from datasets import load_dataset\n","\n","import glob\n","import pickle\n","import re \n","from termcolor import colored\n","from transformers import AutoModelForMaskedLM, AutoTokenizer\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","import torch\n","import math\n","\n","\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZbblmDePioA"},"source":["# import the data-----------------------------------------------------\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVQ5-uYkog5z"},"source":["\"\"\"\n","all_poems_beit_add = '...pd.read_pickle(all_poems_beit_add)\n","\n","import hazm\n","normalizer = hazm.Normalizer(persian_numbers=False)\n","normalized = []\n","\n","for text in all_poems:\n","    \n","    normalized.append(normalizer.normalize(text))\n","\n","x_train, x_val = train_test_split(normalized, shuffle = True, test_size = 0.1)\n","\n","(pd.DataFrame(pd.Series(x_train), columns = ['poetry'])).to_csv(train_path,\n","                                                                index=False)\n","(pd.DataFrame(pd.Series(x_val), columns = ['poetry'])).to_csv(val_path,\n","                                                                index=False)\n","                            \n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxQFuktHFaZb"},"source":["val_path = '.../Data/all_poetry_val_beit.csv'\n","train_path = '.../Data/all_poetry_train_beit.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yEangO7PikA"},"source":["dataset_poetry = load_dataset('csv', data_files={'train': train_path,\n","                                                'test': val_path})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwVFKqsFB-yN"},"source":["model_path = \"HooshvareLab/roberta-fa-zwnj-base\"\n","\n","\n","# model with specific vocab and folder\n","model = AutoModelForMaskedLM.from_pretrained(model_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_path,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaxNe4KhPih5"},"source":["len(tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKn_2r6wCzs7"},"source":["tokenizer.add_tokens(['برآرد', 'برآید', 'وزآن', 'درآمد', \n","                      'بدانگهی', 'نام‌آو',\n","                      'ناآشنا', 'بدخویی', 'براندیشم'])\n","\n","model.resize_token_embeddings(len(tokenizer))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vz_egy7siy1K"},"source":["#tokenizer.encoder('سلام')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4878r7OpPrw"},"source":["# model with specific vocab and folder\n","model_folder_path_Roberta = '.../Pretrained Models/Pretrained on beit/Roberta_0.4_beit/'\n","\n","model = AutoModelForMaskedLM.from_pretrained(model_folder_path_Roberta)\n","tokenizer = AutoTokenizer.from_pretrained(model_folder_path_Roberta,\n","                                               use_fast=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tormnKU_PiK7"},"source":["training_args = TrainingArguments( \n","    \"test-clm\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,  \n","    load_best_model_at_end=True,\n","    num_train_epochs=5\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCsJnppKQRmT"},"source":["def tokenize_function(examples):\n","    return tokenizer(examples['poetry'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHK7vHefQTIM"},"source":["tokenized_datasets = dataset_poetry.map(\n","    tokenize_function, \n","    batched=True, \n","    num_proc=4,\n","    batch_size=512)\n","\n","\n","tokenized_datasets[\"train\"][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoDy9Ci3QU5R"},"source":["tokenized_datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-obWOIRSQWyD"},"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n","                                                mlm_probability=0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhjqzJxQQcjL"},"source":["trainer = Trainer(\n","    model=model, \n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator=data_collator\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yY031QsYDCat"},"source":["trainer.train() # mask 0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iwDammo5BCrd"},"source":["#tokenizer.save_pretrained('.../Pretrained Models/Roberta_0.4_beit/')\n","#model.save_pretrained('.../Pretrained Models/Roberta_0.4_beit/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HeNi-wSgQchN"},"source":["trainer.train( ) # mask 0.6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdFq-9urg5jF"},"source":["#torch.save(model.state_dict(), '.../Pretrained Models/Roberta_0.4_beit/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPTiCD7gqA2a"},"source":["trainer.train() # 0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOhBl4-TQceu"},"source":["eval_results = trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8ZmzzXqDcj8"},"source":["import math\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") # mask 0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKWcCEd6y1dL"},"source":[""],"execution_count":null,"outputs":[]}]}