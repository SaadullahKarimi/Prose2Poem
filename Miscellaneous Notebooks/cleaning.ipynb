{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cleaning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO9f0+tETqQq2KbFWavc796"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"UKnwwtbSAKgc"},"source":["# packages and modules\n","\n","!pip install hazm\n","!pip install https://github.com/sobhe/hazm/archive/master.zip --upgrade\n","!pip install requests nlpaug transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tdz5l4XIEV2n"},"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import string\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pandas as pd\n","import hazm\n","pd.set_option('display.max_rows', 50)\n","pd.set_option('display.max_colwidth', None)\n","from termcolor import colored\n","from itertools import chain\n","import pickle\n","import nlpaug.augmenter.word as naw\n","import pickle\n","from hazm import stopwords_list\n","\n","from transformers import AutoModelForMaskedLM, AutoTokenizer\n","from collections import Counter \n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","from termcolor import colored\n","import ast\n","\n","from sklearn.model_selection import StratifiedKFold\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","from tqdm.notebook import tqdm\n","from itertools import chain\n","\n","from sklearn.preprocessing import LabelEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_hD4YTmEV0Y"},"source":["# the main ProsPoemParallelDataset is constructed of different files\n","# this file has all in one, without the duplicates\n","\n","all_data = pd.read_csv('.../ProsPoemParallelDataset.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9Ie6CQ-EVyZ"},"source":["\n","# cleaning phase 1-----------------------------\n","normalizer = hazm.Normalizer(persian_numbers=False)\n","\n","# all punctuations except . and /\n","punct = re.sub(r'[\\/\\.]', '', string.punctuation) +'،؟»«…'\n","\n","\n","separators = ['خلاصه داستان:.*',\n","              ' حاصل کلام .*',\n","              'در یک کلام.*',\n","              ' حاصل معنی:.*',\n","              'خلاصه کلام:.*',\n","              'حاصل اینکه.*',\n","              'معنی بیت*',\n","              '[\\.|؟] یعنی.*', \n","              'چنانکه* .*(:)+',\n","              '\\.( )*خلاصه.*',\n","              \n","              'به عبارتی دیگر، .*',\n","              '\\. حاصل سخن اینکه.*',\n","              ' حاصل کلام: *',\n","              '\\. بنابراین .*',\n","              'منظور بیت: .*',\n","              '\\.( )*بنابراین.*',\n","              'حاصل مقصود اینکه.*',\n","              'چنانکه در آیه.*'\n","\n","\n","              ]\n","\n","# extra patterns\n","remove_patterns = [r'\\[.*\\]*',  r'\\(.*\\)', r'-.*',\n","                    r'–.*', r'شرح و تفسیر غزل \\S*', r'\\. \\. ',\n","                    '\\. \\S*( )*=',\n","                    '\\. \\S* \\S*( )*=',\n","                    r'ص \\S*', 'شرح و تفسیر بیت \\S*',\n","                    'از این‌ها که بگذریم', 'یعنی نطفۀ موسی', 'چنانکه شاعر می‌فرماید:.*',\n","                    ' دنیا ترک کن یعنی', ' پیر مغان یعنی',\n","                    'ابیات پیشین مقدمه‌ای است برای این بیت و بیت بعد\\.', \n","                    ' اوضاع و احوالی که در بیت قبل گفته شد به اضافه احوال او در مصراع اول این بیت',\n","                    'حضرت مولانا در اینجا پاسخ می‌دهد: ', ' چنانکه گفت اند:.*',\n","                    'آن مار زهرآگین او را نیش زد و .*', \n","                    'چه هر روز عرفای کاملی .*', \n","                    'چنانکه عمر خیام می‌فرماید:.*', \n","                    '\\*.*',\n","                    '_ مولانا در این بخش جلیل.*',\n","                    '.* دلیل مولانا در رد ادلۀ متکلمان بدین قرار است: ',\n","                    'احرام = آهنگ حج کردن .*',\n","                    '\\. تمثیل:', \n","                    'این بیداری که مورد نکوهش مولاناست .*',\n","                    'و خلاصه همه حواس منقطع می شود',\n","                    'طبرسی گوید:.*', 'این سیاهکاران.*',\n","                    'مثلا شیر پشمین ساختگی را برای گدائی بکار می‌برند، یعنی',\n","                    ' خلاصۀ داستان:.*',\n","                    'آن دوست گفت: همراه.*',\n","                    '.* تشبیه شده است و می‌گوید:',\n","                    'یکی از اعیان ناس نقل می‌کرد:.*',\n","                    'نام او صد و سی بار در قرآن کریم.*', 'در سال ۱۶۵ هجری در .*', \n","                    ' ابن الوقت در مثنوی به دو معنی آمده.*', \n","                    'حسام الدین ادامه داد:', ' دجال به معنی دروغگوی.*',\n","                    ' و گاه به معنی.*', \n","                    ' اکبر آبادی معتقد است.*', \n","                    'بعضی از مریدان گفتند که.*', \n","                    '.* حاصل این که:',\n","                    '.*حاصل بیت:',\n","                    'از نظم و انتظام خارج شده یعنی ',\n","                    '.* پس با این وصف،',\n","                    'دل خرابی می‌کند یعنی اسرار را فاش می‌سازد.',\n","                    '.*اما صورت تمثیل:', \n","                    'توضیح بیشتر در.*',\n","                    ' مسلمان‌گر بدانستی که بت چیست.*',\n","                    ' حیرت بر دو نوع است.*',\n","                    'این بخش جلیل،',\n","                    r'حضرت علی به آن پهلوان',\n","                    '«بطر به معنی.*',\n","                    '«زنان بدکار شایسته.*',\n","                    ' و قبل از آغاز حکایت.*',\n","                    'خطاب به دل گوید: ',\n","                    ' بس گریزند از بلا سوی بلا / بس جهند از مار سوی اژدها',\n","                    'بود نور نبی خورشید اعظم.*',\n","                    'قانع و راضی باش / بی آرام چیزی.*',\n","                    'جور دشمن چه کند‌گر نکشد طالب دوست / گنج و مار و گل و خار و غم و شادی به هم اند',\n","                    'شعرا سفتن را به کنایه.*',\n","                   'سید کفافی ج ۲',\n","                   r'[0-9]'\n","\n","                                        \n","\n","                    ]\n","\n","def cleaning_1(text):\n","    \"\"\"First set of cleaning and removing extra text\"\"\"\n","\n","    text = normalizer.normalize(text)\n","\n","\n","    text = re.sub('ایتجا', 'اینجا', text)\n","    \n","\n","\n","    sub_1 = [' فرسی ', 'ایتجا', ' شخ ']\n","    sub_2 = ['فارسی', ' اینجا', ' شخص ']\n","\n","\n","    for patt in  remove_patterns + separators:   \n","        text = re.sub(patt, ' ', text)\n","\n","    for i in range(len(sub_1)):\n","        text = re.sub(sub_1[i], sub_2[i], text)\n","\n","\n","    text =  re.sub('^ ', '', text)\n","    text =  re.sub(' $', '', text)\n","    text =  re.sub(r' */ *', ' / ', text)\n","\n","    text =  re.sub(r' \\. \\.', '\\.', text)\n","    text =  re.sub(' +\\s', ' ', text)\n","\n","    text =  re.sub(' \\.$', '\\.', text)\n","    text =  re.sub('^ *\\. *', '', text)\n","\n","    text =  re.sub('[۱۲۳۴۵۶۷۸۹۰]', '', text)\n","\n","    \n","    # replace extra spaces or tabs with only one space\n","    text =  re.sub(r' \\s+', ' ', text)\n","\n","\n","    tex =  re.sub(r'([\\/\\.])', r'\\1', text)\n","    \n","    tex =  re.sub(r'\\s+', ' ', text)\n","\n","    text = re.sub('^ ', '', text)\n","\n","    text = re.sub(' $', '', text)\n","\n","\n","    text = re.sub(r' \\. \\.', r'.', text)\n","\n","    text = re.sub('و *$', '', text)\n","    \n","    # remove punc\n","    text = text.translate(str.maketrans('', '', punct))\n","\n","    \n","\n","    return text\n","    \n","\n","all_data.loc[:, 'text'] = all_data.loc[:, 'text'].apply(lambda x:cleaning_1(x))\n","all_data.loc[:, 'poetry'] = all_data.loc[:, 'poetry'].apply(lambda x:cleaning_1(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYhUbiV0EVwo"},"source":["# check for very short sentences (plain text)\n","\n","selected = []\n","for i in range(len(all_data_final['text'])):\n","    if len(all_data_final.loc[i, 'text'])<32:\n","        print(i)\n","        print(all_data_final.loc[i, 'text'])\n","        #print(all_data_not_cleaned.loc[i,'text'])\n","        \n","        selected.append(i)\n","print(colored(f'length of matches: {len(selected)}', 'blue'))\n","\n","all_data_final.drop(index = selected, inplace=True)\n","all_data_final.reset_index(drop=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0nb5-liMAf4i"},"source":["# exporting the cleaned dataset\n","\n","#all_data_final.to_csv(None, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2unbEcyBHLov"},"source":["# Augmentin the poetry"]},{"cell_type":"code","metadata":{"id":"9m7V-nrzGs74"},"source":["# this model has been pretrained on couplets\n","# and will be used for augmenting poetry\n","\n","bent_trained_path = None\n","\n","persian_stw = stopwords_list()\n","aug = naw.ContextualWordEmbsAug(model_path=bent_trained_path,\n","                                action=\"substitute\",\n","                                stopwords=persian_stw,\n","                                temperature=0.4, \n","                                device='cuda' if tf.test.gpu_device_name() else 'cpu',\n","                                top_p=0.99,\n","                                top_k=100, \n","                                aug_min=1,\n","                                #tokenizer =tokenizer_source,\n","                                aug_p=0.2\n","                                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5eJRcvgGtGj"},"source":["# augmenting the poetry\n","\n","for i in range(len(all_data)):\n","    \n","    all_data = pd.concat([all_data, \n","                           pd.DataFrame({'poetry': [aug.augment(all_data.loc[i,'poetry'], n= 2)],  \n","                                          'text': all_data.loc[i,'text']})],\n","                           axis=0)\n","    if i % 100 == 0:\n","        print(i)\n","        with open(None, 'wb') as f:\n","            pickle.dump(all_data, f)\n","\n","all_data.reset_index(inplace=True, drop=True)\n","all_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NC1T5J9UGtEP"},"source":["unwanted_records = []\n","\n","# put records with more than one translation (augmented) into two rows\n","\n","for i in range(len(all_data)):\n","    if type(all_data.loc[i,'poetry']) == list: \n","\n","        all_data = pd.concat([all_data, \n","                             pd.DataFrame({'poetry': all_data.loc[i, 'poetry'], \n","                                           'text': all_data.loc[i,'text']})],\n","                             axis=0)\n","        unwanted_records.append(i)\n","\n","all_data.drop(unwanted_records, inplace=True)\n","all_data.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Z0d_t7hGtCe"},"source":["# removing the records with generated unknown tokens\n","\n","unk_t = []\n","for i in range(len(all_data)):\n","    if re.search('\\[UNK\\]',  all_data.loc[i, 'poetry']):\n","        print(all_data.loc[i, 'poetry'])\n","        unk_t.append(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ljm73OQZGs_x"},"source":["# augmenting the text\n","model_check_point = 'HooshvareLab/bert-fa-base-uncased'\n","\n","\n","# for text use high temperature\n","persian_stw = stopwords_list()\n","aug = naw.ContextualWordEmbsAug(model_path=model_check_point,\n","                                action=\"substitute\",\n","                                stopwords=persian_stw,\n","                                temperature=0.6, \n","                                device='cuda' if tf.test.gpu_device_name() else 'cpu',\n","                                top_p=0.8,\n","                                top_k=150, \n","                                aug_min=3,\n","                                #tokenizer =tokenizer_source,\n","                                aug_p=0.4\n","                                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbCE1FXAGs56"},"source":["for i in range(len(all_data)):\n","    \n","    all_data = pd.concat([all_data, \n","                           pd.DataFrame({'poetry': all_data.loc[i, 'poetry'],  \n","                                          'text': [aug.augment(all_data.loc[i,'text'], n= 1)]})],\n","                           axis=0)\n","    if i % 100 == 0:\n","        print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR6SNsu0Gs3w"},"source":["all_data.reset_index(inplace=True,drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKG_L4YRNykf"},"source":["# Cleaning other files we use"]},{"cell_type":"code","metadata":{"id":"GEDb2U6HN0wj"},"source":["# Motaradefs(synonyms) ----------------------------------\n","\n","mot = pd.read_pickle(None)\n","df = pd.DataFrame({'word':list(mot.keys()), 'syn':list(mot.values())})\n","\n","# remove stopwords\n","\n","stw_ = stopwords_list()\n","banned_words = set(stw_)\n","\n","unwanted = []\n","for i in range(len(df)):\n","    for word in banned_words:\n","        if df.loc[i, 'word'] == word:\n","            print(colored(word, 'blue'))\n","            print(df.loc[i, 'syn'])\n","            unwanted.append(i)\n","\n","df.drop(unwanted, inplace=True)\n","df.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzdSvYnlN0tL"},"source":["def clean(mot_list):\n","    \"\"\"\n","    cleaning each unwanted letter or number in the words\n","    \"\"\"\n","    unw = []\n","\n","    for i in range(len(mot_list)):\n","\n","        \n","        if len(mot_list[i])==0: unw.append(i)\n","\n","\n","\n","    if len(unw)!=0:\n","        for index in sorted(set(unw), reverse=True):\n","            del mot_list[index]\n","\n","        \n","    else :\n","        for i in range(len(mot_list)):\n","            for j in range(len(mot_list[i])):\n","\n","                text = normalizer.normalize(mot_list[i][j])\n","                text = re.sub(r'[۱|۲|۳|۴|۵|۶|۷|۸|۹|۰|؟|!|.|،|,|?]', '', text)\n","                mot_list[i][j] = text \n","                \n","\n","    return mot_list\n","\n","df.loc[:, 'syn'] = df.loc[:, 'syn'].apply(lambda x:clean(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cK9qf4ISN0vE"},"source":["# removing exessive nouns with no synonyms\n","\n","remove_these = []\n","for i in range(len(df)):\n","    if len(df.loc[i, 'syn']) ==0:\n","        print(i)\n","        print(df.loc[i, :])\n","        remove_these.append(i)\n","\n","df.drop(unwanted, inplace=True)\n","df.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVi_RimdN0rD"},"source":["all_dict = {\n","    'word':[],\n","    'synonym':[],\n","    'antonym':[]\n","}\n","\n","for i in range(len(df)):\n","    all_dict['word'].append(df.loc[i, 'word'])\n","    \n","    all_dict['synonym'].append(df.loc[i, 'syn'][0])\n","    try: all_dict['antonym'].append(df.loc[i, 'syn'][1])\n","    except: all_dict['antonym'].append('nothing')\n","\n","all_mots = pd.DataFrame(all_dict)\n","all_mots.to_csv(None)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dYJGkgIQ13G"},"source":["# Cleaning and augmenting the Semantic affinity dataset"]},{"cell_type":"code","metadata":{"id":"OoMxJx6WRXnm"},"source":["# main file\n","gherab = pd.read_pickle('.../Gherabat.pickle')\n","\n","# additional files for each class\n","erfani = pd.read_csv('.../erfani.txt', header=None)[0].tolist()\n","khodae = pd.read_csv('.../khodae.txt', header=None)[0].tolist()\n","akhlaghi = pd.read_csv('.../akhlagh.txt', header=None)[0].tolist()\n","eshghi = pd.read_csv('.../love.txt', header=None)[0].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6ZZU6YKSGuF"},"source":["import hazm\n","normalizer = hazm.Normalizer(persian_numbers=False)\n","cleaned = []\n","\n","\n","sub_1 = ['هٔ', 'أ', 'ئ', '»', 'ؤ', 'ازآن', 'هرآنچه', 'ازآنک', 'زورآور', 'وآن',\n","         'وآی', ' ٔ', 'ء', '؟']\n","sub_2 = ['ه', 'ا', 'ی', '', 'و', 'از آن', 'هر آنچه', 'از آنک', 'زور آور', 'و آن',\n","         'و آی', '', '', ''] # has been modified\n","\n","for j in range(len(all_poems)):\n","    \n","\n","    text = all_poems[j]\n","    text = normalizer.normalize(text)\n","\n","    for i in range(len(sub_1)):\n","        text = re.sub(sub_1[i], sub_2[i], text)\n","    \n","    cleaned.append(text)\n","\n","# verse into couplet\n","akhlaghi_b=[]\n","for i in range(2, len(akhlaghi), 2):\n","    akhlaghi_b.append(' / '.join(akhlaghi[i-2:i]))\n","\n","\n","love_b=[]\n","for i in range(2, len(eshghi), 2):\n","    love_b.append(' / '.join(eshghi[i-2:i]))\n","\n","erfani_b=[]\n","for i in range(2, len(erfani), 2):\n","    erfani_b.append(' / '.join(erfani[i-2:i]))\n","\n","khodae_b=[]\n","for i in range(2, len(khodae), 2):\n","    khodae_b.append(' / '.join(khodae[i-2:i]))\n","\n","# all additional files in one datafile\n","df = pd.DataFrame({\n","    'topic': ['akhlaghi']*len(akhlaghi_b) + ['eshghi']*len(love_b) + ['erfan']*len(erfani_b) + ['khodavand']*len(khodae_b),\n","    'poetry': akhlaghi_b + love_b + erfani_b + khodae_b\n","})\n","\n","# primary file\n","\n","df_main = pd.DataFrame({\n","    'topic': [],\n","    'poetry': []\n","})\n","\n","for k,v in gherab.items():\n","    df_main = pd.concat([df_main, pd.DataFrame({\n","    'topic': k,\n","    'poetry': v\n","})], axis=0)\n","\n","\n","\n","# concatenating the semantica affinity files\n","concatenated = pd.concat([df, df_main], axis=0)\n","concatenated.reset_index(inplace=True, drop=True)\n","\n","# removing duplicates and very short texts\n","concatenated.drop(concatenated[[True if len(i)<32 else False for i in concatenated['poetry']]].index, inplace=True)\n","concatenated.drop_duplicates(inplace=True, subset='poetry')\n","concatenated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkZyDlTgS669"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SlG5Sg2XQ1t2"},"source":["# augmented dataset for the semantic affinity classification\n","concat_path = None\n","\n","concatenated.to_csv(concat_path)\n","concatenated = pd.read_csv(concat_path)\n","print('length of the augmented dataset for automatic eval: ', colored( f\"{len(concatenated):,}\", 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5UKY7CeQ1qm"},"source":["# we use the bert model trained on poetry for augmenting the poetry\n","# in this dataset\n","\n","persian_stw = stopwords_list()\n","aug = naw.ContextualWordEmbsAug(model_path='mitra-mir/BERT-Persian-Poetry',\n","                                action=\"substitute\",\n","                                stopwords=persian_stw,\n","                                temperature=0.3, \n","                                device='cuda' if tf.test.gpu_device_name() else 'cpu',\n","                                top_p=0.99,\n","                                top_k=100, \n","                                aug_min=1,\n","                                #tokenizer =tokenizer_source,\n","                                aug_p=0.2\n","                                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NfJP-_pRtTd"},"source":["concatenated.reset_index(inplace=True, drop=True)\n","for i in range(len(concatenated)):\n","    \n","    concatenated = pd.concat([concatenated, \n","                           pd.DataFrame({'poetry': [aug.augment(concatenated.loc[i,'poetry'], n= 1)],  \n","                                          'topic': concatenated.loc[i,'topic']})],\n","                           axis=0)\n","    if i % 100 == 0:\n","        print(i)\n","concatenated.reset_index(inplace=True, drop=True)\n","\n","# removing augmented records with [UNK]\n","unk_t = []\n","for i in range(len(concatenated)):\n","    try:\n","        if re.search('\\[UNK\\]',  concatenated.loc[i, 'poetry']):\n","            print(concatenated.loc[i, 'poetry'])\n","            unk_t.append(i)\n","\n","    except: print(concatenated.loc[i, 'poetry'])\n","\n","concatenated = pd.read_csv('.../concatenated_augmented.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6siFmwtERtRN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YiU6l22gGzyv"},"source":["### A few experiments along the way"]},{"cell_type":"code","metadata":{"id":"f2QRLsllAljh"},"source":["# needed data\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","hafez = pd.read_pickle('.../hafez.pickle')\n","molana = pd.read_pickle('.../molana.pickle')\n","sadi = pd.read_pickle('.../saedi.pickle')\n","added = pd.read_csv('.../data_added_1.csv')\n","\n","\n","\n","hafez = pd.DataFrame([i[3:] for i in hafez], columns=['poetry', 'text'])\n","molana = pd.DataFrame([i[3:] for i in molana], columns=['poetry', 'text'])\n","sadi = pd.DataFrame([i[3:] for i in sadi], columns=['poetry', 'text'])\n","\n","\n","dfs = [hafez, sadi, molana]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ag4F9oNRC8d5"},"source":["all_data = pd.concat([hafez, molana, sadi, added], axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2geIfhkEJkK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZDfEGF8GmAI"},"source":["# exporting the cleaned dataset\n","\n","#all_data_final.to_csv(None, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iddH5rWVGlpI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lzBp74XTCt3I"},"source":["print('removing duplication...')\n","molana = molana[~molana.duplicated()]\n","molana.reset_index(drop=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDyjA7OzA9hq"},"source":["all_data = all_data[~all_data.duplicated()]\n","all_data.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOmE9_9wCAXO"},"source":["added.drop(columns=['Unnamed: 0'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KkEsQxcCX2Q"},"source":["for i in range(len(all_data)):\n","    if re.search('چه کسی می تواند بخشش بی نهایت خداوند را بشمارد', all_data.loc[i, 'text']):\n","        print('ff')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nU1BsXAaClih"},"source":["matches = []\n","for i in range(len(all_data_final)):\n","    if re.search( '^بنابراین', all_data_final.loc[i, 'text'],):\n","        print(i)\n","        print( all_data_final.loc[i, 'text'])\n","        matches.append(i)\n","print('# of matches : ', colored(len(matches), 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8YsoNuAGU6g"},"source":["# removing unwanted long sentences\n","selected = []\n","for i in range(len(all_data_final['text'])):\n","    if len(all_data_final.loc[i, 'text'])>295:\n","        print(i)\n","        print(all_data_final.loc[i, 'text'])\n","        #print(all_data_not_cleaned.loc[i,'text'])\n","        \n","        selected.append(i)\n","print(colored(f'length of matches: {len(selected)}', 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulFsuLKyGU4o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jmaQKnsGU2o"},"source":[""],"execution_count":null,"outputs":[]}]}