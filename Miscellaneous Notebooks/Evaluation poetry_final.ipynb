{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Evaluation poetry_final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"az3076DwW36z"},"source":["# Packages"]},{"cell_type":"code","metadata":{"id":"Lr6DDIz2c8rM"},"source":["!pip install hazm\n","!pip install transformers\n","!pip install rouge\n","! pip install rouge-metric"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2i3d1ucc_-B"},"source":["# Import required packages\n","\n","import numpy as np\n","import pandas as pd\n","from collections import Counter \n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","from transformers import AutoModelForMaskedLM, AutoTokenizer, TFAutoModelForMaskedLM\n","\n","import hazm\n","\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","from tqdm.notebook import tqdm\n","from itertools import chain\n","import gc\n","from sklearn.preprocessing import LabelEncoder\n","import os\n","import re\n","import json\n","import copy\n","import collections\n","import collections\n","from rouge import Rouge \n","\n","from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n","cc = SmoothingFunction()\n","\n","from transformers import BertConfig, BertTokenizer, TFAutoModel \n","from transformers import TFBertModel, TFBertForSequenceClassification, AutoTokenizer\n","from transformers import glue_convert_examples_to_features\n","import math \n","import tensorflow as tf\n","from termcolor import colored\n","import ast\n","from rouge_metric import PyRouge\n","\n","\n","from sklearn.model_selection import StratifiedKFold\n","from statistics import mean\n","import glob\n","import os\n","import glob\n","import string\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option('display.max_rows', 50)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJsShc-ndDWH"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z445cKxXdESv"},"source":["# augmented dataset for the semantic affinity classification\n","concat_path = None\n","concatenated = pd.read_csv(concat_path)\n","print('length of the augmented dataset for automatic eval: ', colored( f\"{len(concatenated):,}\", 'blue'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fdm3DN21eXpQ"},"source":["# Create the classifier"]},{"cell_type":"code","metadata":{"id":"0o-r3r3cdEO_"},"source":["# use the roberta pretrained model for classifying poetries\n","\n","MODEL_NAME_OR_PATH = '.../Roberta_0.4_beit'\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, use_fast=True)\n","\n","max_len = max([len(tokenizer.encode(txt)) for txt in concatenated['poetry'].to_list()]) +5\n","max_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xeNnnNg9iss8"},"source":["def transformer_encoder_(texts):\n","\n","    dict = tokenizer.batch_encode_plus(texts,\n","        max_length = max_len,\n","        add_special_tokens = True,\n","        padding='max_length', \n","        truncation=True)\n","\n","    inputs = {\n","      'input_word_ids': tf.ragged.constant(dict.input_ids).to_tensor(),\n","      'input_mask': tf.ragged.constant(dict.attention_mask).to_tensor()\n","      }\n","\n","    return inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFB0vwHsdEM3"},"source":["def build_custome_model():\n","    \n","    bert_encoder = TFAutoModel.from_pretrained(MODEL_NAME_OR_PATH+'/TensorFlow')\n","    \n","    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","\n","\n","    embedding = bert_encoder([input_word_ids, input_mask])[0]\n","\n","    clf_output = embedding[:,0,:]\n","\n","    net = tf.keras.layers.Dense(32, activation='tanh')(clf_output)\n","    net = tf.keras.layers.Dropout(0.6)(net)\n","\n","\n","    # output = tf.keras.layers.Dense(5, activation='softmax')(net)\n","\n","    output = tf.keras.layers.Dense(4 , activation='softmax')(net)\n","\n","    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","    \n","    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zODbBxPdEKn"},"source":["model = build_custome_model()\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IV151d7IdEIm"},"source":["k = 4\n","kfold = StratifiedKFold(n_splits = k, shuffle = True)\n","\n","df_train = concatenated.copy()\n","\n","labels = df_train.loc[:, 'topic']\n","\n","poetry = df_train.loc[:, 'poetry']\n","\n","# preparing the label\n","le = LabelEncoder()\n","le.fit(labels)\n","labels_ = le.transform(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYel_i3kdEGm"},"source":["for i, (train_idx, val_idx) in enumerate(kfold.split(poetry.tolist(), labels_), 1): \n","    print(colored(f'fold {i}', 'blue'))\n","    epoch_evaluation = {}\n","\n","    \n","\n","    \n","    train_input = transformer_encoder_(\n","        poetry[train_idx].tolist()\n","        )\n","    \n","    #print(np.array(train_input))\n","    validation_input = transformer_encoder_(\n","        poetry[val_idx].tolist()\n","        )\n","    \n","    history = model.fit(x = train_input, y = labels_[train_idx],\n","                        validation_data= (validation_input, labels_[val_idx]),\n","                        epochs = 1,\n","                        verbose = 1,\n","                        batch_size = 16, \n","                        #callbacks=[cp_callback]\n","                        )\n","    gc.collect()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLlwkv3Mhx_t"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MHzzXthhx80"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kn46YeNrhyrc"},"source":["# Evaluate"]},{"cell_type":"markdown","metadata":{"id":"g9e9CWallUFD"},"source":["# BLEU"]},{"cell_type":"code","metadata":{"id":"c1FFKpz4dD9n"},"source":["molana = pd.read_pickle('.../Data/molana.pickle')\n","molana = pd.DataFrame([i[3:] for i in molana], columns=['poetry', 'text'])\n","\n","reference = [[i.split() for i in molana.loc[:, 'poetry'].values.tolist()]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgYAxgg5dD7H"},"source":["normalizer = hazm.Normalizer()\n","\n","# all_data_not_cleaned = pd.concat([sadi, molana, hafez, added], axis = 0).reset_index(drop=True)\n","\n","# all punctuations except . and /\n","punct = re.sub(r'[\\/]', '', string.punctuation) +'،؟»«…'\n","\n","def cleaning(text):\n","    \"\"\"First set of cleaning and removing extra text\"\"\"\n","\n","    text = normalizer.normalize(text)\n","    # remove punc\n","    text = text.translate(str.maketrans('', '', punct))\n","    return text\n","\n","molana.loc[:, 'poetry'] =  molana.loc[:, 'poetry'].apply(lambda x: cleaning(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVDHgkrRdD4w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUjCxtfapovb"},"source":["def find_my_bleu(text, w):\n","\n","    candidates_ = [text.split()]\n","    #print(candidates_)\n","    return corpus_bleu(reference, candidates_, weights=w, \n","                                        smoothing_function=cc.method4)\n","\n","def get_final_bleu(output_df):\n","\n","    print('Started calculating the bleu scores...')\n","    output_df.loc[:, 'bleu_1'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_bleu(t, (1, 0, 0, 0)) for t in x])\n","    output_df.loc[:, 'bleu_2'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_bleu(t, (0, 1, 0, 0)) for t in x])\n","    output_df.loc[:, 'bleu_3'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_bleu(t, (0, 0, 1, 0)) for t in x])\n","\n","\n","    print('Now the average score...')\n","    output_df.loc[:, 'bleu_3_mean'] = output_df.loc[:, 'bleu_3'].apply(lambda x:np.mean(x))\n","    output_df.loc[:, 'bleu_2_mean'] = output_df.loc[:, 'bleu_2'].apply(lambda x:np.mean(x))\n","    output_df.loc[:, 'bleu_1_mean'] = output_df.loc[:, 'bleu_1'].apply(lambda x:np.mean(x))\n","\n","    print('mean bleu_3 score: ', np.mean(output_df.loc[:, 'bleu_3_mean']))\n","    print('mean bleu_2 score: ', np.mean(output_df.loc[:, 'bleu_2_mean']))\n","    print('mean bleu_1 score: ', np.mean(output_df.loc[:, 'bleu_1_mean']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0z2y1n7W--u"},"source":["#col_name = 'poetry_generated_Seq2Seq_with_Att'\n","#col_name = 'poetry_generated_MHA'\n","# col_name = 'poetry_generated_Seq2Seq_GRU\n","\n","def get_bleu_phase_1(df, col_name = 'poetry_generated_MHA'):\n","\n","    bleu_1_list = []\n","    bleu_2_list = []\n","    bleu_3_list = []\n","\n","    for i in range(len(df)):\n","\n","        refrence_ = base_df.loc[i, 'poetry_ground_truth'].split()\n","        \n","\n","        # bleu unigrams\n","        #print(df.loc[i, col_name])\n","        candidate = df.loc[i, col_name]\n","        bleu_1_list.append(sentence_bleu(refrence_, \n","                                        candidate,\n","                                        weights=(1, 0, 0, 0), \n","                                        smoothing_function=cc.method4))\n","        \n","        \n","        bleu_2_list.append(sentence_bleu(refrence_, \n","                                        candidate,\n","                                        weights=(0, 1, 0, 0), \n","                                        smoothing_function=cc.method4))\n","        \n","        bleu_3_list.append(sentence_bleu(refrence_, \n","                                        candidate,\n","                                        weights=(0, 0, 1, 0), \n","                                        smoothing_function=cc.method4))\n","        \n","    print(colored('BLEU-1 : ', 'blue'), np.mean(bleu_1_list))\n","    print(colored('BLEU-2 : ', 'blue'), np.mean(bleu_2_list))\n","    print(colored('BLEU-3 : ', 'blue'), np.mean(bleu_3_list))\n","\n","    hyps, refs = map(list, zip(*[[df.loc[i, col_name],\n","                                 df.loc[i, 'poetry_ground_truth']] for i in range(len(df))]))\n","    rouge = Rouge()\n","\n","    scores = rouge.get_scores(hyps, refs, avg=True)\n","\n","    print(colored('\\nROUGE : \\n', 'blue'), scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1PyRrrEukFB"},"source":["# Rouge"]},{"cell_type":"code","metadata":{"id":"-pKk2Ld1o-SC"},"source":["reference_rouge = [i.split() for i in molana.loc[:, 'poetry'].values.tolist()]\n","rouge = PyRouge(rouge_n=(1, 2), rouge_l=True, rouge_w=False, rouge_s=False, rouge_su=False)\n","\n","def find_my_rouge(text):\n","    hypotheses = [[text.split()]]\n","    #print(hypotheses)\n","    score = rouge.evaluate_tokenized(hypotheses, [[reference_rouge]])\n","    #print(score)\n","    return score\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8N2iUP1jo-M9"},"source":["def get_short_rouge(list_dicts):\n","\n","    \"\"\" get the mean of all generated poetries for each record\"\"\"\n","    l_r = 0\n","    l_p = 0\n","    l_f = 0\n","\n","    one_r = 0\n","    one_p  = 0\n","    one_f  = 0\n","\n","    two_r  = 0\n","    two_p  = 0\n","    two_f  = 0\n","    \n","    for d in list_dicts:\n","        \n","        \n","        one_r += d['rouge-1']['r']\n","        one_p += d['rouge-1']['p']\n","        one_f += d['rouge-1']['f']\n","\n","\n","        two_r += d['rouge-2']['r']\n","        two_p += d['rouge-2']['p']\n","        two_f += d['rouge-2']['f']\n","        \n","        l_r += d['rouge-l']['r']\n","        l_p += d['rouge-l']['p']\n","        l_f += d['rouge-l']['f']\n","\n","    length = len(list_dicts)\n","\n","    return {'rouge-1': {'r': one_r/length , 'p': one_p/length , 'f': one_f/length},\n","            'rouge-2': {'r': two_r/length, 'p': two_p/length, 'f': two_f/length},\n","            'rouge-l': {'r': l_r/length, 'p': l_p/length , 'f': l_f/length}\n","            }\n","\n","def get_overal_rouge_mean(output_df):\n","    print('Started getting the overall rouge of each record...')\n","    output_df.loc[:, 'rouge_mean'] = output_df.loc[:, 'rouge'].apply(lambda x: get_short_rouge(x))\n","    print('Started getting the overall rouge of all record...')\n","    l_r = 0\n","    l_p = 0\n","    l_f = 0\n","\n","    one_r = 0\n","    one_p  = 0\n","    one_f  = 0\n","\n","    two_r  = 0\n","    two_p  = 0\n","    two_f  = 0\n","\n","    for i in range(len(output_df)):\n","        d = output_df.loc[i, 'rouge_mean']\n","        \n","        one_r += d['rouge-1']['r']\n","        one_p += d['rouge-1']['p']\n","        one_f += d['rouge-1']['f']\n","\n","\n","        two_r += d['rouge-2']['r']\n","        two_p += d['rouge-2']['p']\n","        two_f += d['rouge-2']['f']\n","        \n","        l_r += d['rouge-l']['r']\n","        l_p += d['rouge-l']['p']\n","        l_f += d['rouge-l']['f']\n","\n","    length = len(output_df)\n","    print('overall rouge scores: ')\n","    print({'rouge-1': {'r': one_r/length , 'p': one_p/length , 'f': one_f/length},\n","                'rouge-2': {'r': two_r/length, 'p': two_p/length, 'f': two_f/length},\n","                'rouge-l': {'r': l_r/length, 'p': l_p/length , 'f': l_f/length}\n","                })\n","    return output_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBUe7s1oo9wd"},"source":["#get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehr9JpnXlRtr"},"source":["# Semantic Similarity"]},{"cell_type":"code","metadata":{"id":"OWrYoCgFhx5O"},"source":["def predict(sent):\n","\n","    pred = model.predict(transformer_encoder_([sent]))\n","    pred_label = le.inverse_transform([np.argmax(pred)])[0]\n","    \n","    return pred_label\n","\n","\n","def clean_phase_1(txt):\n","\n","    txt = re.sub('<sep>', ' / ', txt)\n","    txt = re.sub('(<start>)|(<end>)', '', txt)\n","\n","    return txt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syFD8-rwdEEI"},"source":["def import_results_final_approach(model_name='BERT', on='beit'):\n","\n","\n","    \"\"\"\n","    import the results from the heuristic and final section\n","    \"\"\"\n","    \n","\n","    if on == 'beit':\n","        \n","\n","        elif  model_name=='BERT V3':\n","            folder_path = r'.../Bert V3/Final_BERT_V3_gh_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","\n","        elif  model_name=='Roberta':\n","            folder_path = '.../Roberta/Final_Roberta_gh_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","        \n","        elif  model_name=='Distilbert':\n","            folder_path = '.../DistilBERT/Final_Distilbert_gh_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","\n","        elif  model_name=='Albert':\n","            folder_path = '.../Albert/Final_Albert_gh_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","        \n","        #if model_name=='BERT V2':\n","         #   folder_path = r'/content/drive/MyDrive/Paper 4 - Poetry Generation/Pretrained Models/Pretrained on beit/BERT_V2_0.4_beit/'\n","        \n","\n","\n","    else:\n","\n","\n","        if model_name=='BERT V3':\n","            folder_path = '.../Bert V3/Final_BERT_V3_gh_last_Verse_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","        \n","        elif  model_name=='Albert':\n","            folder_path = '.../Albert/Final_Albert_gh_Verse_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","        \n","        elif  model_name=='Roberta':\n","            folder_path = '.../Roberta/Final_Roberta_gh_Verse_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","        \n","        elif  model_name=='Distilbert':\n","            folder_path = '.../DistilBERTFinal_DistilBERT_gh_Verse_M_poetry_format_20_numOfKeywords_4_mesra_first_elements_num_4_mesra_second_elements_num_3_beam_depth_100_first.csv'\n","\n","\n","    return folder_path\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNqgYjiEkFS7"},"source":["def choose_one(output_df, base_df):\n","    \"\"\"\n","    choose one label for all the generated poetry \n","    for each plain text\n","    \"\"\"\n","\n","    for i in range(len(output_df)):\n","\n","        labels_d = output_df.loc[i, 'prediction_for_output']\n","        generated_p = sorted(labels_d, reverse=True)\n","        gt = base_df.loc[i, 'prediction_for_gt']\n","\n","\n","        if gt in generated_p: final_label = gt\n","\n","        else: final_label = generated_p[0]\n","\n","        output_df.loc[i, 'chosen_prediction_for_output'] = final_label\n","    return output_df\n","\n","def automatic_eval(output_df, base_df):\n","    \"\"\" \n","    predict the predicted poetry labels\n","    \"\"\"\n","\n","    output_df.loc[:, 'prediction_for_output'] = output_df.loc[:, 'final_predicted_verses'].apply (lambda x: [predict(sent) for sent in x])\n","    output_df.loc[:, 'prediction_for_output'] = output_df.loc[:, 'prediction_for_output'].apply(lambda x: dict(Counter(x)))\n","\n","    \n","    output_df = choose_one(output_df, base_df)\n","\n","    gherabat = len(output_df[output_df['chosen_prediction_for_output']==base_df['prediction_for_gt']])/len(output_df) \n","    print('semantic similarity is observed in', colored(f\"{gherabat:.2f}\", 'blue'), 'percent of cases.')\n","\n","    return output_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgruk3z7zDPQ"},"source":["# get one file ground truths as the base format for comparison\n","base = import_results_final_approach(model_name='Roberta', on='beit')\n","base_df = pd.read_csv(base)\n","base_df.drop(columns=['heuristics', 'final_predicted_verses'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOtWSq9YyaoP"},"source":["# clean the multihead results\n","base_df.loc[:, 'poetry_generated_MHA'] = base_df.loc[:, 'poetry_generated_MHA'].apply(lambda x: clean_phase_1(x))\n","# predict the ground truth \n","base_df.loc[:, 'prediction_for_gt'] = base_df.loc[:, 'poetry_ground_truth'].apply (lambda x: predict(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9hf05k47Oj6"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"SABC-m5toYEB"},"source":["## Roberta"]},{"cell_type":"markdown","metadata":{"id":"-sAuENHYr8eZ"},"source":["### beit"]},{"cell_type":"code","metadata":{"id":"A52PFMmhdEAH"},"source":["folder_path = import_results_final_approach(model_name='Roberta', on='beit')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fa7lBJ0Q44EG"},"source":["# get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D7GeSTOU45Gw"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITn-Vl827oIF"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"181hCZIzsekB"},"source":["### verse"]},{"cell_type":"code","metadata":{"id":"usuXiEMXseYZ"},"source":["\n","folder_path = import_results_final_approach(model_name='Roberta', on='verse')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVqUHZAA7lGV"},"source":["get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6u6QYp-71PO"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYFo98Or798t"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kN-SitUzoaT5"},"source":["## Distilbert"]},{"cell_type":"markdown","metadata":{"id":"Z9togPPVsAo4"},"source":["### beit"]},{"cell_type":"code","metadata":{"id":"LjxnJkG9ocEp"},"source":["\n","folder_path = import_results_final_approach(model_name='Distilbert', on='beit')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxmEoOJ78CBd"},"source":["get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30FbPEXQ8B3V"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpVS6YoU8BxM"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0JEVKpYsIik"},"source":["### verse"]},{"cell_type":"code","metadata":{"id":"cb4fHTocsK5p"},"source":["\n","\n","folder_path = import_results_final_approach(model_name='Distilbert', on='verse')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m85KkLp08EOb"},"source":["get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60PROfoT8ELt"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RzdR8-i8EI1"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GjOzrlE-olpR"},"source":["## Albert"]},{"cell_type":"markdown","metadata":{"id":"k8nIWA6ZsCvy"},"source":["### beit"]},{"cell_type":"code","metadata":{"id":"WZEKjBHJok0y"},"source":["\n","folder_path = import_results_final_approach(model_name='Albert', on='beit')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7K2n7jU8FUV"},"source":["get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktrCuMBT8FP9"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jehQHCWe8FNF"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRCQoqwcsPVq"},"source":["### verse"]},{"cell_type":"code","metadata":{"id":"gSPmARiHsP0Q"},"source":["\n","folder_path = import_results_final_approach(model_name='Albert', on='verse')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1nJxCQ18GGo"},"source":["get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHBMejDT8GEK"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgQjCsYA8GB2"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hChj97sDotZq"},"source":["## Bert"]},{"cell_type":"markdown","metadata":{"id":"i_Q856WnsD_Q"},"source":["### beit"]},{"cell_type":"code","metadata":{"id":"SfDQK-8iokwq"},"source":["\n","folder_path = import_results_final_approach(model_name='BERT V3', on='beit')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbDTVCTy8HHf"},"source":["get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKwxi-1d8HFB"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WeTs3oHh8HCp"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vlBK7wisMEx"},"source":["### verse"]},{"cell_type":"code","metadata":{"id":"upr9L9QcsL69"},"source":["\n","folder_path = import_results_final_approach(model_name='BERT V3', on='verse')\n","output_df = pd.read_csv(folder_path)\n","output_df.loc[:, 'final_predicted_verses'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x : ast.literal_eval(x))\n","\n","output_df = automatic_eval(output_df, base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"yl5dQ_wm8IJ0"},"source":["get_final_bleu(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPP__8GM8IHq"},"source":["output_df.loc[:, 'rouge'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_rouge(t) for t in x])\n","output_df = get_overal_rouge_mean(output_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"ECKfMu8l8IFV"},"source":["output_df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKySMs2HnxdE"},"source":["# Eval for phase |Models"]},{"cell_type":"markdown","metadata":{"id":"IoRJWOXlxAaH"},"source":["## MultiheadAttention"]},{"cell_type":"code","metadata":{"id":"10XLItjUxIvf"},"source":["col_name = 'poetry_generated_MHA'\n","\n","base_df.loc[:, col_name+'_pred'] = base_df.loc[:, col_name].apply (lambda x: predict(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqI9eH0vw_3w"},"source":["gherabat = len(base_df[base_df[col_name+'_pred']==base_df['prediction_for_gt']])/len(base_df) \n","print('semantic similarity is observed in', colored(f\"{gherabat:.2f}\", 'blue'), 'percent of cases.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7i29c_rH975e"},"source":["get_bleu_phase_1(base_df, col_name = col_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liAZ7Y2UDMzc"},"source":["base_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F6cTGlnTF8hD"},"source":["# Seq2Seq with attention"]},{"cell_type":"code","metadata":{"id":"pH3Uy9JJI4QC"},"source":["all(base_df.loc[: , 'poetry_ground_truth'] == output_df.loc[:99, 'poetry_ground_truth'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKN6-CZKKSM7"},"source":["base_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3GtFZfiHBZq"},"source":["col_name = 'poetry_generated_Seq2Seq_with_Att'\n","\n","\n","output_df = pd.read_csv(None)\n","assert all(base_df.loc[: , 'poetry_ground_truth'] == output_df.loc[:99, 'poetry_ground_truth'])\n","output_df = output_df.loc[:99, :]\n","\n","\n","output_df.loc[:, col_name] = output_df.loc[:, col_name].apply(lambda x: clean_phase_1(x))\n","output_df.loc[:, col_name+'_pred'] = output_df.loc[:, col_name].apply (lambda x: predict(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVO0iiJhF8S1"},"source":["output_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYHXO9PiF8Qm"},"source":["gherabat = len(output_df[output_df[col_name+'_pred']==base_df['prediction_for_gt']])/len(base_df) \n","print('semantic similarity is observed in', colored(f\"{gherabat:.2f}\", 'blue'), 'percent of cases.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eC7r--2sF8Ob"},"source":["get_bleu_phase_1(output_df, col_name = col_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ywbfQW-KvOp"},"source":["# Normal RNN"]},{"cell_type":"code","metadata":{"id":"kYzBZx0IShNp"},"source":["output_df = pd.read_csv(None)\n","\n","output_df.loc[:99, 'poetry_ground_truth']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8HT4WIEKvCS"},"source":["col_name = 'poetry_generated_Seq2Seq_GRU'\n","\n","\n","assert all(base_df.loc[: , 'poetry_ground_truth'] == output_df.loc[:99, 'poetry_ground_truth'])\n","output_df = output_df.loc[:99, :]\n","\n","\n","output_df.loc[:, col_name] = output_df.loc[:, col_name].apply(lambda x: clean_phase_1(x))\n","output_df.loc[:, col_name+'_pred'] = output_df.loc[:, col_name].apply (lambda x: predict(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"So7KozZZKu_m"},"source":["gherabat = len(output_df[output_df[col_name+'_pred']==base_df['prediction_for_gt']])/len(base_df) \n","print('semantic similarity is observed in', colored(f\"{gherabat:.2f}\", 'blue'), 'percent of cases.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPV3ez2gKu8C"},"source":["get_bleu(output_df, col_name = col_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRYYUfSswr1R"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jo6klqpBJnO5"},"source":[""],"execution_count":null,"outputs":[]}]}