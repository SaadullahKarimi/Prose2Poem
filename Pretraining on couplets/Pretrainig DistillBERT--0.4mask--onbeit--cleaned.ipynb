{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pretrainig DistillBERT--0.4mask--onbeit--cleaned.ipynb","provenance":[{"file_id":"1lqUNWyrlul8MEpubwHr0ouI8WYwephLk","timestamp":1620223467902},{"file_id":"14ULMHrN3BTjI9w-sT_QFKHOs0HlpRBCT","timestamp":1620115289180},{"file_id":"1gxR-mFjOxEI3hGvaMdddO87_hNCyrBR4","timestamp":1619791942785}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fPeuh14MO3yj"},"source":["!pip install datasets transformers\n","!pip install hazm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HrioHhGPiqJ"},"source":["from datasets import load_dataset\n","\n","import glob\n","import pickle\n","import re \n","from termcolor import colored\n","from transformers import AutoModelForMaskedLM, AutoTokenizer\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","import torch\n","import math\n","\n","\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZbblmDePioA"},"source":["# import the data-----------------------------------------------------\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4Fhl-GbxE_7"},"source":["# creating the dataset"]},{"cell_type":"code","metadata":{"id":"DVQ5-uYkog5z"},"source":["\"\"\"\n","all_poems_beit_add = '.../Data/beits_joined_cleaned.pickle'\n","all_poems = pd.read_pickle(all_poems_beit_add)\n","\n","import hazm\n","normalizer = hazm.Normalizer(persian_numbers=False)\n","normalized = []\n","\n","for text in all_poems:\n","    \n","    normalized.append(normalizer.normalize(text))\n","\n","x_train, x_val = train_test_split(normalized, shuffle = True, test_size = 0.1)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sqeHKgAkoYb8"},"source":["\"\"\"\n","(pd.DataFrame(pd.Series(x_train), columns = ['poetry'])).to_csv(train_path,\n","                                                                index=False)\n","(pd.DataFrame(pd.Series(x_val), columns = ['poetry'])).to_csv(val_path,\n","                                                                index=False)\n","                                                                \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFgKQgzexHzw"},"source":["# Reading the dataset and training"]},{"cell_type":"code","metadata":{"id":"pxQFuktHFaZb"},"source":["val_path = '.../Data/all_poetry_val_beit.csv'\n","train_path = '.../Data/all_poetry_train_beit.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yEangO7PikA"},"source":["dataset_poetry = load_dataset('csv', data_files={'train': train_path,\n","                                                'test': val_path})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwVFKqsFB-yN"},"source":["model_path = \"HooshvareLab/distilbert-fa-zwnj-base\"\n","\n","\n","# model with specific vocab and folder\n","model = AutoModelForMaskedLM.from_pretrained(model_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_path,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaxNe4KhPih5"},"source":["len(tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKn_2r6wCzs7"},"source":["tokenizer.add_tokens(['برآرد', 'برآید', 'وزآن', 'درآمد', \n","                      'بدانگهی', 'نام‌آو',\n","                      'ناآشنا', 'بدخویی', 'براندیشم'])\n","\n","model.resize_token_embeddings(len(tokenizer))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4878r7OpPrw"},"source":["model_folder_path_distilbert = '.../Pretrained Models/Pretrained on beit/DistilBERT_0.4_beit/'\n","# model with specific vocab and folder\n","\n","model = AutoModelForMaskedLM.from_pretrained(model_folder_path_distilbert)\n","tokenizer = AutoTokenizer.from_pretrained(model_folder_path_distilbert,\n","                                               use_fast=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tormnKU_PiK7"},"source":["training_args = TrainingArguments( \n","    \"test-clm\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,  \n","    load_best_model_at_end=True,\n","    num_train_epochs=5\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCsJnppKQRmT"},"source":["def tokenize_function(examples):\n","    return tokenizer(examples['poetry'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHK7vHefQTIM"},"source":["tokenized_datasets = dataset_poetry.map(\n","    tokenize_function, \n","    batched=True, \n","    num_proc=5,\n","    batch_size=512)\n","\n","\n","tokenized_datasets[\"train\"][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-obWOIRSQWyD"},"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n","                                                mlm_probability=0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhjqzJxQQcjL"},"source":["trainer = Trainer(\n","    model=model, \n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator=data_collator\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRnJ6F3RpMh6"},"source":["trainer.train() # mask 0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOhBl4-TQceu"},"source":["eval_results = trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8ZmzzXqDcj8"},"source":["import math\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") # mask 0.4 Distilbert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PPi-R5arVl5"},"source":["\"\"\"\n","model.save_pretrained(model_folder_path_bert_beit_07)\n","tokenizer.save_pretrained(model_folder_path_bert_beit_07)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59dRV7ycLtZ9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8R-L0zUqg5dt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2f7wfLXqA-K"},"source":["# Phase ||"]},{"cell_type":"code","metadata":{"id":"vnSDYjGvsvv5"},"source":["model_folder_path_bert_beit_07 = r'.../Pretrained Models/bert_beit_07/'\n","\n","\n","# model with specific vocab and folder\n","model = AutoModelForMaskedLM.from_pretrained(model_folder_path_bert_beit_07)\n","tokenizer = AutoTokenizer.from_pretrained(model_folder_path_bert_beit_07,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJfJFhxNuxZ7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylQyQ9lZr3IK"},"source":["len(tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNSLQ31Jr3Gp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZpae0fcr3EA"},"source":["training_args = TrainingArguments( \n","    \"test-clm\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01, \n","    load_best_model_at_end=True, \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfmEKjYYuZ-_"},"source":["def tokenize_function(examples):\n","    return tokenizer(examples['poetry'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DysIS9xvr3Bo"},"source":["tokenized_datasets = dataset_poetry.map(\n","    tokenize_function, \n","    batched=True, \n","    num_proc=4, \n","    batch_size=512)\n","\n","\n","tokenized_datasets[\"train\"][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnKu5GeWr2-g"},"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n","                                                mlm_probability=0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCl4O9nItV41"},"source":["trainer = Trainer(\n","    model=model, \n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator=data_collator,  \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDtomXZAr9xp"},"source":["trainer.train() # mask 0.7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ms93RKfftJFR"},"source":["model_folder_path_bert_beit_07 = r'.../Pretrained Models/bert_beit_07_6Epochs/'\n","\n","model.save_pretrained(model_folder_path_bert_beit_07)\n","tokenizer.save_pretrained(model_folder_path_bert_beit_07)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-im8n3Jx7rJW"},"source":["model_folder_path_bert_beit_07 = r'.../Pretrained Models/bert_beit_07_6Epochs/'\n","\n","\n","# model with specific vocab and folder\n","model = AutoModelForMaskedLM.from_pretrained(model_folder_path_bert_beit_07)\n","tokenizer = AutoTokenizer.from_pretrained(model_folder_path_bert_beit_07,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZsrtpuo7D8W"},"source":["trainer.train() # mask 0.7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pg6R50Jo7vfb"},"source":["model_folder_path_bert_beit_07_2 = r'.../Pretrained Models/bert_beit_07_2/'\n","\n","\n","model.save_pretrained(model_folder_path_bert_beit_07_2)\n","tokenizer.save_pretrained(model_folder_path_bert_beit_07_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5a_wk7MTBDP"},"source":[""],"execution_count":null,"outputs":[]}]}