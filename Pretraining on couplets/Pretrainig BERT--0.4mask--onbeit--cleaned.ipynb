{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pretrainig BERT--0.4mask--onbeit--cleaned.ipynb","provenance":[{"file_id":"1lqUNWyrlul8MEpubwHr0ouI8WYwephLk","timestamp":1620223467902},{"file_id":"14ULMHrN3BTjI9w-sT_QFKHOs0HlpRBCT","timestamp":1620115289180},{"file_id":"1gxR-mFjOxEI3hGvaMdddO87_hNCyrBR4","timestamp":1619791942785}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fPeuh14MO3yj"},"source":["!pip install datasets transformers\n","!pip install hazm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HrioHhGPiqJ"},"source":["from datasets import load_dataset\n","\n","import glob\n","import pickle\n","import re \n","from termcolor import colored\n","from transformers import AutoModelForMaskedLM, AutoTokenizer\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","import torch\n","import math\n","\n","\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZbblmDePioA"},"source":["# import the data-----------------------------------------------------\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVQ5-uYkog5z"},"source":["\"\"\"\n","all_poems_beit_add = '.../Data/beits_joined_cleaned.pickle'\n","all_poems = pd.read_pickle(all_poems_beit_add)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1k97bmDW5sB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIJsxDsrrZL6"},"source":["\"\"\"\n","import hazm\n","normalizer = hazm.Normalizer(persian_numbers=False)\n","normalized = []\n","\n","for text in all_poems:\n","    \n","    normalized.append(normalizer.normalize(text))\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8PdbSqaPimJ"},"source":["#x_train, x_val = train_test_split(normalized, shuffle = True, test_size = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxQFuktHFaZb"},"source":["val_path = '.../Data/all_poetry_val_beit.csv'\n","train_path = '.../Data/all_poetry_train_beit.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sqeHKgAkoYb8"},"source":["\"\"\"\n","(pd.DataFrame(pd.Series(x_train), columns = ['poetry'])).to_csv(train_path,\n","                                                                index=False)\n","(pd.DataFrame(pd.Series(x_val), columns = ['poetry'])).to_csv(val_path,\n","                                                                index=False)\n","                                                                \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5boXLKkqoYZ7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1JpOabAoYXr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yEangO7PikA"},"source":["dataset_poetry = load_dataset('csv', data_files={'train': train_path,\n","                                                'test': val_path})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwVFKqsFB-yN"},"source":["\n","model_checkpoint_bert_V3 = 'HooshvareLab/bert-fa-zwnj-base'\n","# model with specific vocab and folder\n","model = AutoModelForMaskedLM.from_pretrained(model_checkpoint_bert_V3)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_bert_V3,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaxNe4KhPih5"},"source":["len(tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKn_2r6wCzs7"},"source":["tokenizer.add_tokens(['برآرد', 'برآید', 'وزآن', 'درآمد', \n","                      'بدانگهی', 'نام‌آو',\n","                      'ناآشنا', 'بدخویی', 'براندیشم'])\n","\n","model.resize_token_embeddings(len(tokenizer))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vz_egy7siy1K"},"source":["#tokenizer.encoder('سلام')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4878r7OpPrw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BinOBk8h4d0"},"source":["tokenizer.vocab.keys()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ur4IHIiTIM6-"},"source":["tokenizer.vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tormnKU_PiK7"},"source":["training_args = TrainingArguments( \n","    \"test-clm\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,  \n","    load_best_model_at_end=True,\n","    num_train_epochs=5\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCsJnppKQRmT"},"source":["def tokenize_function(examples):\n","    return tokenizer(examples['poetry'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHK7vHefQTIM"},"source":["tokenized_datasets = dataset_poetry.map(\n","    tokenize_function, \n","    batched=True, \n","    num_proc=4,\n","    batch_size=512)\n","\n","\n","tokenized_datasets[\"train\"][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4r55cYvrFyT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_zFmOBpqSWi"},"source":["tokenizer.unk_token_id "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoDy9Ci3QU5R"},"source":["tokenized_datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-obWOIRSQWyD"},"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n","                                                mlm_probability=0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhjqzJxQQcjL"},"source":["trainer = Trainer(\n","    model=model, \n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator=data_collator\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yY031QsYDCat"},"source":["trainer.train() # mask 0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iwDammo5BCrd"},"source":["tokenizer.save_pretrained('.../Pretrained Models/BERT_0.4_beit/')\n","model.save_pretrained('.../Pretrained Models/BERT_0.4_beit/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRnJ6F3RpMh6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59dRV7ycLtZ9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOhBl4-TQceu"},"source":["eval_results = trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8ZmzzXqDcj8"},"source":["import math\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") # mask 0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XESZflqefqjv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIhXy3zHfqdp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbVwnIqAQcZx"},"source":["sequence = 'هزار <mask> هست از نیای او افضل'\n","\n","\n","top_k=5\n","ids_ = tokenizer.encode(sequence,\n","                            return_tensors=\"pt\",\n","                            add_special_tokens=False)\n","\n","position = torch.where(ids_ == tokenizer.mask_token_id)\n","\n","positions_list = position[1].numpy().tolist()\n","ids_ = ids_.to('cuda')\n","predictions_ids = {}\n","predictions_detokenized_sents = {}\n","\n","for i in range(len(positions_list)):\n","    predictions_ids[i] = []\n","    predictions_detokenized_sents[i] = []\n","    # where do we have mask?\n","    # print(i)\n","\n","\n","    \n","    # if it was the first prediction, \n","    # just go on and predict the first predictions\n","    \n","\n","    if i==0:\n","        model_logits = model(ids_)['logits'][0][positions_list[0]]\n","        top_k_tokens = torch.topk(model_logits, top_k, dim=0).indices.tolist()\n","\n","        for j in range(len(top_k_tokens)):\n","            #print(j)\n","            ids_[0][positions_list[0]] = top_k_tokens[j]\n","            pred = tokenizer.decode(ids_[0])\n","\n","            # append the sentences and ids of this masked\n","            # token\n","            predictions_ids[i].append(ids_)\n","            predictions_detokenized_sents[i].append(pred)\n","\n","\n","\n","    # if we already have some predictions, go on and fill the rest\n","    # of the masks by continuing the previous predictions\n","    if i!=0:\n","        for pred in predictions_ids[i-1]:\n","            print(pred)\n","            # get the logits\n","            model_logits = model(pred)['logits'][0][positions_list[i]]\n","            # get the top 5 of this prediction and masked token\n","            top_k_tokens = torch.topk(model_logits, top_k, dim=0)\\\n","            .indices.tolist()\n","\n","            for top_id in top_k_tokens:\n","                # print(top_id)\n","                \n","                ids_[0][positions_list[i]] = top_id\n","                pred = tokenizer.decode(ids_[0])\n","\n","                # append the sentences and ids of this masked\n","                # token\n","                predictions_ids[i].append(ids_)\n","                predictions_detokenized_sents[i].append(pred)\n","\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58fmIUmfQcW5"},"source":["predictions_detokenized_sents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3ROeLIrQcUr"},"source":["import time, psutil\n","uptime = time.time() - psutil.boot_time()\n","print('How much I used?\\n {} hours, and {:.2f} minutes '.format(uptime//3600, uptime%60))\n","remain = 24*60*60 - uptime\n","print('How much is remaining?\\n {} hours, and {:.2f} minutes '.format(remain//3600, remain%60))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PPi-R5arVl5"},"source":["model.save_pretrained(model_folder_path_bert_beit_07)\n","tokenizer.save_pretrained(model_folder_path_bert_beit_07)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2f7wfLXqA-K"},"source":["# Phase ||"]},{"cell_type":"code","metadata":{"id":"vnSDYjGvsvv5"},"source":["model_folder_path_bert_beit_07 = r'.../Pretrained Models/bert_beit_07/'\n","\n","\n","# model with specific vocab and folder\n","model = AutoModelForMaskedLM.from_pretrained(model_folder_path_bert_beit_07)\n","tokenizer = AutoTokenizer.from_pretrained(model_folder_path_bert_beit_07,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJfJFhxNuxZ7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylQyQ9lZr3IK"},"source":["len(tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNSLQ31Jr3Gp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZpae0fcr3EA"},"source":["training_args = TrainingArguments( \n","    \"test-clm\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01, \n","    load_best_model_at_end=True, \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfmEKjYYuZ-_"},"source":["def tokenize_function(examples):\n","    return tokenizer(examples['poetry'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DysIS9xvr3Bo"},"source":["tokenized_datasets = dataset_poetry.map(\n","    tokenize_function, \n","    batched=True, \n","    num_proc=4, \n","    batch_size=512)\n","\n","\n","tokenized_datasets[\"train\"][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnKu5GeWr2-g"},"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n","                                                mlm_probability=0.7, )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCl4O9nItV41"},"source":["trainer = Trainer(\n","    model=model, \n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator=data_collator,  \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDtomXZAr9xp"},"source":["trainer.train() # mask 0.7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ms93RKfftJFR"},"source":["model_folder_path_bert_beit_07 = r'.../Pretrained Models/bert_beit_07_6Epochs/'\n","\n","model.save_pretrained(model_folder_path_bert_beit_07)\n","tokenizer.save_pretrained(model_folder_path_bert_beit_07)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-im8n3Jx7rJW"},"source":["model_folder_path_bert_beit_07 = r'.../Pretrained Models/bert_beit_07_6Epochs/'\n","\n","\n","# model with specific vocab and folder\n","model = AutoModelForMaskedLM.from_pretrained(model_folder_path_bert_beit_07)\n","tokenizer = AutoTokenizer.from_pretrained(model_folder_path_bert_beit_07,\n","                                               use_fast=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZsrtpuo7D8W"},"source":["trainer.train() # mask 0.7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pg6R50Jo7vfb"},"source":["model_folder_path_bert_beit_07_2 = r'.../Pretrained Models/bert_beit_07_2/'\n","\n","\n","model.save_pretrained(model_folder_path_bert_beit_07_2)\n","tokenizer.save_pretrained(model_folder_path_bert_beit_07_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5a_wk7MTBDP"},"source":[""],"execution_count":null,"outputs":[]}]}